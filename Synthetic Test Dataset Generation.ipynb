{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API KEY를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "# API KEY 정보로드\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIzaSyC06D**********\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.environ['GOOGLE_API_KEY'][:10]+'*'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith 추적을 시작합니다.\n",
      "[프로젝트명]\n",
      "Synthetic Data Generation\n"
     ]
    }
   ],
   "source": [
    "from langchain_teddynote import logging\n",
    "\n",
    "logging.langsmith(\"Synthetic Data Generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "\n",
    "# 문서 로더 생성\n",
    "# loader = PDFPlumberLoader(\"data/Prompt_Tuning.pdf\")\n",
    "loader = PDFPlumberLoader(\"data/Domain-specialized-LLM-Financial-fine-tuning-and-utilization-method-using-Mistral-7B.pdf\")\n",
    "\n",
    "# 문서 로딩\n",
    "docs = loader.load()\n",
    "\n",
    "# 목차, 끝 페이지 제외\n",
    "docs = docs[3:-1]\n",
    "\n",
    "# 문서의 페이지수\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'도메인 특화 LLM: Mistral 7B를 활용한 금융 업무분야 파인튜닝 및 활용 방법\\n\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\U000f080f\\n그리고 LLM의 파인튜닝에 대하여 상세하게 알아 Human Feedback)시키는 학습과정을 거치면서 대화\\n보고, 본 논문에서 다루는 금융분야에서의 언어 형으로 발전시킨 GPT-3.5모델을 적용하여 사람과\\n모델 적용 영역으로 나누어 설명한다. 유사한 자연어를 생성하도록 학습된 AI 챗봇인\\nChatGPT를 공개하여 출시 2개월만에 월간 이용자가\\n2.1. LLM(Large Language Model)의 개요 1억명을 넘기며 많은 관심을 받았다(정천수, 2023b).\\nChatGPT같은 오픈된 도메인에 대한 챗봇 대화\\n최근 몇 년 동안, LLM은 NLP(자연어 처리) 분야\\n에서는 ChatGPT와 LLM모델과의 Prompt로 인터\\n에서 차별화된 발전을 이루고 있다. Transformer\\n페이스를 하게 되는데, 이때 인터페이스를 하기 위한\\n구조를 기반으로 한 BERT (Bidirectional Encoder\\n어플리케이션 사이에 사용하는 매개변수를 적용\\nRepresentations from Transformers), GPT (Generative\\n할 때 주의해야 할 정보는 개인정보에 관한 것은\\nPre-trained Transformer) 등의 모델은 대량의 텍스트\\n회피해야 한다. 개발자가 동의 없이 사용자로부터\\n데이터를 사전 훈련하여 다양한 자연어 태스크에\\n데이터를 수집하고 사용하는 것을 방지하는 법이\\n적용할 수 있는 강력한 표현을 학습하였다. 이들\\n이미 있지만, 실제 생활에서 사용자는 개발자가\\n모델은 텍스트의 문맥을 파악하고, 다양한 문법\\n데이터를 얼마나 많이 가져오고, 해당 데이터가\\n구조와 의미적 관계를 이해하는 데에 우수한 성\\n어디에 있는지 알기 어렵기 때문이다(Jeong and\\n과를 보여주고 있다. 생성형 AI(Generative AI)는\\nJeong, 2022). 또한 ChatGPT에서 대화 시에는 질\\n방대한 양의 학습된 데이터모델을 바탕으로 텍\\n문이나 요청인 Prompt를 얼마나 자세하게 전달\\n스트, 이미지, 오디오, 비디오와 같은 새로운 콘\\n하느냐에 따라 완성도 높은 답변을 얻을 수 있기\\n텐츠를 생성할 수 있는 인공지능의 한 형태이다\\n때문에 LLM으로부터 프롬프트 입력 값들의 조합을\\n(Jeong, 2023). 2024년 2월에는 OpenAI에서 Text-\\n찾는 작업을 탐구하는 프롬프트 엔지니어링도\\nto-Video모델인 SORA를 발표해 비디오를 쉽게\\n중요한 요소로 작용한다(정천수, 2023c). 특히 금융\\n생성할 수 있는 모델을 출시하였다(OpenAI, 2024).\\n분야에서는 고객 응대 챗봇을 통한 최신 정보 제\\n또한 NLP 분야중 챗봇은 NLU(Natural Language\\n공의 중요성에 대두되며, LLM의 정보 제한성과\\nUnderstanding) 기술의 발전으로 Context 모델과\\n환각(Hallucination) 문제는 이러한 모델들의 도전\\nTransformer 언어모델 활용으로 복잡한 대화 처리\\n과제로 지적되고 있다. 이를 해결하기 위한 접근\\n가 가능하다(정천수, 2023a). 또한 챗봇에 RPA 및\\n방식으로는 새로운 데이터로의 파인튜닝과 프롬\\nOCR 등 타 솔루션과 연계하여 챗봇을 업무에 직접\\n프트 콘텍스트에 직접 정보를 삽입하는 방안이\\n적으로 활용하여 효율성을 높이고 있다(정천수,\\n있으나, 파인튜닝의 경우에 학습을 위한 인프라\\n정지환, 2020). 이렇게 LLM과 생성형 AI는 AI의\\n준비 등 상당한 비용이 발생하며, 모든 정보를\\n딥러닝 안에 포지셔닝하고 있어 딥러닝 기반으\\n프롬프트에 넣어주는 것도 현실적으로 어렵기\\n로 LLM을 활용하여 생성형 AI 서비스를 할 수\\n때문에 이에 대안으로 RAG모델이 제안되었으며,\\n있게 된다(Mayank, 2023; 정천수, 2023d). 2022년\\n<그림 1>과 같이 정보를 벡터 데이터베이스에\\n11월에는 OpenAI에서 GPT-3에 인간 전문가 집\\n저장하고, 필요한 정보를 검색하여 LLM에 전달\\n단이 피드백(RLHF, Reinforcement Learning from\\n하는 방식으로 구현되기도 한다(정천수, 2023d).\\n95\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "for doc in docs:\n",
    "\ttext_org = doc.page_content\n",
    "\tclean_text = re.split(r'\\U000f080f', text_org)[-1]\n",
    "\tdoc.page_content = clean_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n그리고 LLM의 파인튜닝에 대하여 상세하게 알아 Human Feedback)시키는 학습과정을 거치면서 대화\\n보고, 본 논문에서 다루는 금융분야에서의 언어 형으로 발전시킨 GPT-3.5모델을 적용하여 사람과\\n모델 적용 영역으로 나누어 설명한다. 유사한 자연어를 생성하도록 학습된 AI 챗봇인\\nChatGPT를 공개하여 출시 2개월만에 월간 이용자가\\n2.1. LLM(Large Language Model)의 개요 1억명을 넘기며 많은 관심을 받았다(정천수, 2023b).\\nChatGPT같은 오픈된 도메인에 대한 챗봇 대화\\n최근 몇 년 동안, LLM은 NLP(자연어 처리) 분야\\n에서는 ChatGPT와 LLM모델과의 Prompt로 인터\\n에서 차별화된 발전을 이루고 있다. Transformer\\n페이스를 하게 되는데, 이때 인터페이스를 하기 위한\\n구조를 기반으로 한 BERT (Bidirectional Encoder\\n어플리케이션 사이에 사용하는 매개변수를 적용\\nRepresentations from Transformers), GPT (Generative\\n할 때 주의해야 할 정보는 개인정보에 관한 것은\\nPre-trained Transformer) 등의 모델은 대량의 텍스트\\n회피해야 한다. 개발자가 동의 없이 사용자로부터\\n데이터를 사전 훈련하여 다양한 자연어 태스크에\\n데이터를 수집하고 사용하는 것을 방지하는 법이\\n적용할 수 있는 강력한 표현을 학습하였다. 이들\\n이미 있지만, 실제 생활에서 사용자는 개발자가\\n모델은 텍스트의 문맥을 파악하고, 다양한 문법\\n데이터를 얼마나 많이 가져오고, 해당 데이터가\\n구조와 의미적 관계를 이해하는 데에 우수한 성\\n어디에 있는지 알기 어렵기 때문이다(Jeong and\\n과를 보여주고 있다. 생성형 AI(Generative AI)는\\nJeong, 2022). 또한 ChatGPT에서 대화 시에는 질\\n방대한 양의 학습된 데이터모델을 바탕으로 텍\\n문이나 요청인 Prompt를 얼마나 자세하게 전달\\n스트, 이미지, 오디오, 비디오와 같은 새로운 콘\\n하느냐에 따라 완성도 높은 답변을 얻을 수 있기\\n텐츠를 생성할 수 있는 인공지능의 한 형태이다\\n때문에 LLM으로부터 프롬프트 입력 값들의 조합을\\n(Jeong, 2023). 2024년 2월에는 OpenAI에서 Text-\\n찾는 작업을 탐구하는 프롬프트 엔지니어링도\\nto-Video모델인 SORA를 발표해 비디오를 쉽게\\n중요한 요소로 작용한다(정천수, 2023c). 특히 금융\\n생성할 수 있는 모델을 출시하였다(OpenAI, 2024).\\n분야에서는 고객 응대 챗봇을 통한 최신 정보 제\\n또한 NLP 분야중 챗봇은 NLU(Natural Language\\n공의 중요성에 대두되며, LLM의 정보 제한성과\\nUnderstanding) 기술의 발전으로 Context 모델과\\n환각(Hallucination) 문제는 이러한 모델들의 도전\\nTransformer 언어모델 활용으로 복잡한 대화 처리\\n과제로 지적되고 있다. 이를 해결하기 위한 접근\\n가 가능하다(정천수, 2023a). 또한 챗봇에 RPA 및\\n방식으로는 새로운 데이터로의 파인튜닝과 프롬\\nOCR 등 타 솔루션과 연계하여 챗봇을 업무에 직접\\n프트 콘텍스트에 직접 정보를 삽입하는 방안이\\n적으로 활용하여 효율성을 높이고 있다(정천수,\\n있으나, 파인튜닝의 경우에 학습을 위한 인프라\\n정지환, 2020). 이렇게 LLM과 생성형 AI는 AI의\\n준비 등 상당한 비용이 발생하며, 모든 정보를\\n딥러닝 안에 포지셔닝하고 있어 딥러닝 기반으\\n프롬프트에 넣어주는 것도 현실적으로 어렵기\\n로 LLM을 활용하여 생성형 AI 서비스를 할 수\\n때문에 이에 대안으로 RAG모델이 제안되었으며,\\n있게 된다(Mayank, 2023; 정천수, 2023d). 2022년\\n<그림 1>과 같이 정보를 벡터 데이터베이스에\\n11월에는 OpenAI에서 GPT-3에 인간 전문가 집\\n저장하고, 필요한 정보를 검색하여 LLM에 전달\\n단이 피드백(RLHF, Reinforcement Learning from\\n하는 방식으로 구현되기도 한다(정천수, 2023d).\\n95\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'data/Domain-specialized-LLM-Financial-fine-tuning-and-utilization-method-using-Mistral-7B.pdf',\n",
       " 'file_path': 'data/Domain-specialized-LLM-Financial-fine-tuning-and-utilization-method-using-Mistral-7B.pdf',\n",
       " 'page': 3,\n",
       " 'total_pages': 29,\n",
       " 'Author': 'MS',\n",
       " 'CreationDate': \"D:20240328014510+09'00'\",\n",
       " 'Creator': 'PScript5.dll Version 5.2.2',\n",
       " 'ModDate': \"D:20240328014510+09'00'\",\n",
       " 'Producer': 'Acrobat Distiller 9.3.2 (Windows)',\n",
       " 'Title': '<C1F6B4C9C1A4BAB8203330B1C731C8A35FC5EBBABB2E687770>',\n",
       " 'rgid': 'PB:379341632_AS:11431281232126100@1711621456089'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metadata 설정(filename 이 존재해야 함)\n",
    "for doc in docs:\n",
    "    doc.metadata[\"filename\"] = doc.metadata[\"source\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "# from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "# from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "\n",
    "# 데이터셋 생성기\n",
    "# llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "# llm = ChatOllama(model=\"dnotitia/dna\")\n",
    "# llm = ChatOllama(model=\"llama3.2\")\n",
    "\n",
    "\n",
    "# 문서 임베딩\n",
    "# embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n",
    "# embeddings = OllamaEmbeddings(model=\"bge-m3\")\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\최인규\\Documents\\RAG_practice\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from ragas.testset import TestsetGenerator\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "# LangChain의 gemini 모델을 LangchainLLMWrapper로 감싸 Ragas와 호환되게 만듭니다.\n",
    "generator_llm = LangchainLLMWrapper(llm)\n",
    "\n",
    "# ragas_embeddings 생성\n",
    "ragas_embeddings = LangchainEmbeddingsWrapper(embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 기존 Default 방식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 지식그래프 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문서 노드 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.graph import KnowledgeGraph\n",
    "from ragas.testset.graph import Node, NodeType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kg = KnowledgeGraph()\n",
    "for doc in docs:\n",
    "    kg.nodes.append(\n",
    "        Node(\n",
    "            type=NodeType.DOCUMENT,\n",
    "            properties={\"page_content\": doc.page_content, \"document_metadata\": doc.metadata}\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KnowledgeGraph(nodes: 25, relationships: 0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform 적용으로 노드 재생성 및 엣지(relationships) 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.transforms import default_transforms, apply_transforms\n",
    "\n",
    "\n",
    "# define your LLM and Embedding Model\n",
    "# here we are using the same LLM and Embedding Model that we used to generate the testset\n",
    "transformer_llm = generator_llm\n",
    "embedding_model = ragas_embeddings\n",
    "\n",
    "trans = default_transforms(documents=docs, llm=transformer_llm, embedding_model=embedding_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HeadlinesExtractor(name='HeadlinesExtractor', filter_nodes=<function default_transforms.<locals>.<lambda> at 0x000001C6DE8353A0>, llm=LangchainLLMWrapper(langchain_llm=ChatOpenAI(...)), merge_if_possible=True, max_token_limit=32000, tokenizer=<Encoding 'o200k_base'>, property_name='headlines', prompt=HeadlinesExtractorPrompt(instruction=Extract the most important max_num headlines from the given text that can be used to split the text into independent sections.Focus on Level 2 and Level 3 headings., examples=[(TextWithExtractionLimit(text='                Introduction\\n                Overview of the topic...\\n\\n                Main Concepts\\n                Explanation of core ideas...\\n\\n                Detailed Analysis\\n                Techniques and methods for analysis...\\n\\n                Subsection: Specialized Techniques\\n                Further details on specialized techniques...\\n\\n                Future Directions\\n                Insights into upcoming trends...\\n\\n                Subsection: Next Steps in Research\\n                Discussion of new areas of study...\\n\\n                Conclusion\\n                Final remarks and summary.\\n                ', max_num=6), Headlines(headlines=['Introduction', 'Main Concepts', 'Detailed Analysis', 'Subsection: Specialized Techniques', 'Future Directions', 'Conclusion']))], language=english), max_num=5),\n",
       " HeadlineSplitter(name='HeadlineSplitter', filter_nodes=<function default_filter at 0x000001C6DECAFB00>, min_tokens=500, max_tokens=1000),\n",
       " SummaryExtractor(name='SummaryExtractor', filter_nodes=<function default_transforms.<locals>.<lambda> at 0x000001C6DE834B80>, llm=LangchainLLMWrapper(langchain_llm=ChatOpenAI(...)), merge_if_possible=True, max_token_limit=32000, tokenizer=<Encoding 'o200k_base'>, property_name='summary', prompt=SummaryExtractorPrompt(instruction=Summarize the given text in less than 10 sentences., examples=[(StringIO(text='Artificial intelligence\\n\\nArtificial intelligence is transforming various industries by automating tasks that previously required human intelligence. From healthcare to finance, AI is being used to analyze vast amounts of data quickly and accurately. This technology is also driving innovations in areas like self-driving cars and personalized recommendations.'), StringIO(text='AI is revolutionizing industries by automating tasks, analyzing data, and driving innovations like self-driving cars and personalized recommendations.'))], language=english)),\n",
       " CustomNodeFilter(name='CustomNodeFilter', filter_nodes=<function default_transforms.<locals>.<lambda> at 0x000001C6DE836C00>, llm=LangchainLLMWrapper(langchain_llm=ChatOpenAI(...)), scoring_prompt=QuestionPotentialPrompt(instruction=Given a document summary and node content, score the content of the node in 1 to 5 range., examples=[], language=english), min_score=2, rubrics={'score1_description': 'The page content is irrelevant or does not align with the main themes or topics of the document summary.', 'score2_description': \"The page content partially aligns with the document summary, but it includes unrelated details or lacks critical information related to the document's main themes.\", 'score3_description': 'The page content generally reflects the document summary but may miss key details or lack depth in addressing the main themes.', 'score4_description': 'The page content aligns well with the document summary, covering the main themes and topics with minor gaps or minimal unrelated information.', 'score5_description': \"The page content is highly relevant, accurate, and directly reflects the main themes of the document summary, covering all important details and adding depth to the understanding of the document's topics.\"}),\n",
       " <ragas.testset.transforms.engine.Parallel at 0x1c6ded0c390>,\n",
       " <ragas.testset.transforms.engine.Parallel at 0x1c6e409cd50>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KnowledgeGraph(nodes: 25, relationships: 0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HeadlinesExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\최인규\\AppData\\Local\\Temp\\ipykernel_18468\\1041252660.py:1: RuntimeWarning: coroutine 'Extractor.generate_execution_plan.<locals>.apply_extract' was never awaited\n",
      "  len(trans[0].generate_execution_plan(kg=kg))\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trans[0].generate_execution_plan(kg=kg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\최인규\\AppData\\Local\\Temp\\ipykernel_18468\\966529865.py:3: RuntimeWarning: coroutine 'Extractor.generate_execution_plan.<locals>.apply_extract' was never awaited\n",
      "  len(a.generate_execution_plan(kg=kg))\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas.testset.transforms.extractors.llm_based import HeadlinesExtractor\n",
    "a = HeadlinesExtractor(llm=generator_llm)\n",
    "len(a.generate_execution_plan(kg=kg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "496"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas.utils import num_tokens_from_string\n",
    "num_tokens_from_string(kg.nodes[1].get_property(\"page_content\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying HeadlinesExtractor:   0%|          | 0/23 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    }
   ],
   "source": [
    "apply_transforms(kg, trans[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg.save(\"test_kg/kg_HeadlineExtractor.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_kg = KnowledgeGraph().load(\"test_kg/kg_HeadlineExtractor.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HeadlinesSpliter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeadlineSplitter(name='HeadlineSplitter', filter_nodes=<function default_filter at 0x000001C6DECAFB00>, min_tokens=500, max_tokens=1000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\최인규\\AppData\\Local\\Temp\\ipykernel_18468\\3840880366.py:1: RuntimeWarning: coroutine 'Splitter.generate_execution_plan.<locals>.apply_split' was never awaited\n",
      "  len(trans[1].generate_execution_plan(kg=load_kg))\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trans[1].generate_execution_plan(kg=load_kg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2.1. LLM(Large Language Model)의 개요',\n",
       " '최근 몇 년 동안, LLM은 NLP(자연어 처리) 분야',\n",
       " '생성형 AI(Generative AI)는',\n",
       " '또한 NLP 분야중 챗봇은 NLU(Natural Language Understanding) 기술의 발전으로',\n",
       " '이렇게 LLM과 생성형 AI는 AI의 딥러닝 안에 포지셔닝하고 있어']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_kg.nodes[0].get_property(\"headlines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "402"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(load_kg.nodes[0].get_property(\"page_content\").split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans[1].filter_nodes = trans[0].filter_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes=[node for node in load_kg.nodes if trans[1].filter_nodes(node)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeadlineSplitter(name='HeadlineSplitter', filter_nodes=<function default_transforms.<locals>.<lambda> at 0x00000160BC7842C0>, min_tokens=500, max_tokens=1000)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(load_kg.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    }
   ],
   "source": [
    "from ragas.testset.transforms.splitters.headline import HeadlineSplitter\n",
    "apply_transforms(load_kg, HeadlineSplitter(filter_nodes=trans[0].filter_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_kg.save(\"test_kg/kg_HeadlineSpliter.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_kg=KnowledgeGraph().load(\"test_kg/kg_HeadlineSpliter.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2.1. LLM(Large Language Model)의 개요',\n",
       " '최근 몇 년 동안, LLM은 NLP(자연어 처리) 분야',\n",
       " '생성형 AI(Generative AI)는',\n",
       " '또한 NLP 분야중 챗봇은 NLU(Natural Language Understanding) 기술의 발전으로',\n",
       " '이렇게 LLM과 생성형 AI는 AI의 딥러닝 안에 포지셔닝하고 있어']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_kg.nodes[0].get_property(\"headlines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KnowledgeGraph(nodes: 55, relationships: 20)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_kg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UUID('026ec017-0f40-4a60-b8e3-ad2f7792f9bd')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_kg.nodes[25].id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UUID('fe875f11-1396-47e0-ba29-aaae374ba8b9')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_kg.relationships[0].source.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Node(id: 026ec0, type: NodeType.DOCUMENT, properties: ['page_content', 'document_metadata', 'headlines'])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_kg.nodes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_doc = [n for n in load_kg.nodes if n.type == NodeType.DOCUMENT]\n",
    "len(node_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_chunk = [n for n in load_kg.nodes if n.type == NodeType.CHUNK]\n",
    "len(node_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2.1.2. 파운데이션 모델(Foundation Model) 및 첫 번째 작업은 LLM이 학습될 리소스인 학습 데\n",
      "LLM 사전학습(Pre-training) 이터 세트를 수집하는 것이다. 데이터는 책, 웹\n",
      "생성형 AI 모델은 어떤 출력을 생성하는가에 사이트, 기사, 공개 데이터세트 등 다양한 소스에서\n",
      "따라서 언어모델, 이미지 모델, 동영상 모델 등 가져올 수 있다. 유능한 LLM을 개발하기 위해\n",
      "을 사용한다. 하지만 현재는 이미지와 텍스트를 사전 학습된 자료로 텍스트 데이터 세트를 사용\n",
      "동시에 학습하는 멀티모달(Multi-modal) 모델들이 한다. 사전 학습된 코퍼스의 소스는 크게 일반 데이\n",
      "하루가 다르게 성능과 기능이 업그레이드고 있고 터와 전문 데이터의 두 가지 유형으로 분류할 수\n",
      "있으며 기초모델로 자리잡아가고 있다(정천수, 있으며 웹 페이지, 서적 및 대화 텍스트와 같은\n",
      "2023d). 파운데이션 모델(Foundation Model)의 데 일반 데이터는 크고 다양하며 접근 가능한 특성\n",
      "이터는 텍스트, 이미지, 음성, 정형데이터, 3D 시 으로 인해 대부분의 LLM에서 활용되며 LLM의\n",
      "그널 등 구분하지 않고 학습에 이용되며 인간의 언어 모델링 및 일반화 능력을 향상시킬 수 있다.\n",
      "창의력과 추론력을 포함한 일을 수행하며 이러한 또한 다국어 데이터, 과학 데이터 및 코드와 같은\n",
      "기초모델은 방대한 양의 데이터를 비지도 학습 보다 전문화된 데이터 세트로 확장하여 LLM에\n",
      "(Unsupervised learning)을 통해 모델을 학습시킨 특정 작업 해결 기능을 부여하는 연구도 발표되고\n",
      "후 배포되어 사용자가 원하는 목적에 맞게 다운 있다(Chowdhery et al., 2023; Nijkamp et al., 2022).\n",
      "스트림 작업에 대해 파인튜닝이나 문맥 내 학습 <그림 3>은 일반적인 LLM의 데이터 수집 및\n",
      "(In-context learning)등과 같은 과정을 거처 완성 사전학습 절차를 보여주고 있다(Zhao et al., 2023).\n",
      "되는 것이 파운데이션 모델이라고 볼 수 있다 모델 학습은 지도 학습(Supervised learning)을 사용\n",
      "(Bommasani, et. al., 2021). 하여 전 처리된 텍스트 데이터에 대해 학습된다.\n",
      "파운데이션 모델 중에서 언어모델로서의 LLM은 또한 모델과 데이터의 크기가 크기 때문에 모델을\n",
      "일반 Text 데이터인 Wikipidia 등 거대한 일반적인 학습하려면 엄청난 계산 능력이 필요하며 학습\n",
      "지식들을 수집하여 자기지도학습(Self-Supervised 시간을 줄이기 위해 모델 병렬화라는 기술이 사\n",
      "Learning)이나 반자기지도학습(Semi-Supervised 용된다. 이렇게 대규모 언어 모델을 처음부터 학\n",
      "Learning)을 사용하여 레이블링되지 않은 상당한 습하려면 상당한 투자가 필요하기 때문에 보다\n",
      "양의 텍스트로 사전 학습된 언어 모델(Pre-trained 경제적인 대안으로 기존 언어 모델을 특정 사용\n",
      "Language Model, PLM)이다. 사전학습하기 위한 사례에 맞게 파인튜닝을 하게 된다(정천수, 2023d).\n",
      "<그림 3> LLM 사전 학습 절차\n",
      "99\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(load_kg.relationships[0].source.get_property(\"page_content\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.2. 파운데이션 모델(Foundation Model) 및 첫 번째 작업은 LLM이 학습될 리소스인 학습 데 LLM 사전학습(Pre-training) 이터 세트를 수집하는 것이다. 데이터는 책, 웹 생성형 AI 모델은 어떤 출력을 생성하는가에 사이트, 기사, 공개 데이터세트 등 다양한 소스에서 따라서 언어모델, 이미지 모델, 동영상 모델 등 가져올 수 있다. 유능한 LLM을 개발하기 위해 을 사용한다. 하지만 현재는 이미지와 텍스트를 사전 학습된 자료로 텍스트 데이터 세트를 사용 동시에 학습하는 멀티모달(Multi-modal) 모델들이 한다. 사전 학습된 코퍼스의 소스는 크게 일반 데이 하루가 다르게 성능과 기능이 업그레이드고 있고 터와 전문 데이터의 두 가지 유형으로 분류할 수 있으며 기초모델로 자리잡아가고 있다(정천수, 있으며 웹 페이지, 서적 및 대화 텍스트와 같은 2023d). 파운데이션 모델(Foundation Model)의 데 일반 데이터는 크고 다양하며 접근 가능한 특성 이터는 텍스트, 이미지, 음성, 정형데이터, 3D 시 으로 인해 대부분의 LLM에서 활용되며 LLM의 그널 등 구분하지 않고 학습에 이용되며 인간의 언어 모델링 및 일반화 능력을 향상시킬 수 있다. 창의력과 추론력을 포함한 일을 수행하며 이러한 또한 다국어 데이터, 과학 데이터 및 코드와 같은 기초모델은 방대한 양의 데이터를 비지도 학습 보다 전문화된 데이터 세트로 확장하여 LLM에 (Unsupervised learning)을 통해 모델을 학습시킨 특정 작업 해결 기능을 부여하는 연구도 발표되고 후 배포되어 사용자가 원하는 목적에 맞게 다운 있다(Chowdhery et al., 2023; Nijkamp et al., 2022). 스트림 작업에 대해 파인튜닝이나 문맥 내 학습 <그림 3>은 일반적인 LLM의 데이터 수집 및 (In-context learning)등과 같은 과정을 거처 완성 사전학습 절차를 보여주고 있다(Zhao et al., 2023). 되는 것이 파운데이션 모델이라고 볼 수 있다 모델 학습은 지도 학습(Supervised learning)을 사용 (Bommasani, et. al., 2021). 하여 전 처리된 텍스트 데이터에 대해 학습된다. 파운데이션 모델 중에서 언어모델로서의 LLM은 또한 모델과 데이터의 크기가 크기 때문에 모델을 일반 Text 데이터인 Wikipidia 등 거대한 일반적인 학습하려면 엄청난 계산 능력이 필요하며 학습 지식들을 수집하여 자기지도학습(Self-Supervised 시간을 줄이기 위해 모델 병렬화라는 기술이 사 Learning)이나 반자기지도학습(Semi-Supervised 용된다. 이렇게 대규모 언어 모델을 처음부터 학 Learning)을 사용하여 레이블링되지 않은 상당한 습하려면 상당한 투자가 필요하기 때문에 보다 양의 텍스트로 사전 학습된 언어 모델(Pre-trained 경제적인 대안으로 기존 언어 모델을 특정 사용 Language Model, PLM)이다.\n"
     ]
    }
   ],
   "source": [
    "print(load_kg.relationships[0].target.get_property(\"page_content\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SummaryExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SummaryExtractor(name='SummaryExtractor', filter_nodes=<function default_transforms.<locals>.<lambda> at 0x00000296E0FA00E0>, llm=LangchainLLMWrapper(langchain_llm=ChatOpenAI(...)), merge_if_possible=True, max_token_limit=32000, tokenizer=<Encoding 'o200k_base'>, property_name='summary', prompt=SummaryExtractorPrompt(instruction=Summarize the given text in less than 10 sentences., examples=[(StringIO(text='Artificial intelligence\\n\\nArtificial intelligence is transforming various industries by automating tasks that previously required human intelligence. From healthcare to finance, AI is being used to analyze vast amounts of data quickly and accurately. This technology is also driving innovations in areas like self-driving cars and personalized recommendations.'), StringIO(text='AI is revolutionizing industries by automating tasks, analyzing data, and driving innovations like self-driving cars and personalized recommendations.'))], language=english))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.transforms.extractors.llm_based import SummaryExtractor, SummaryExtractorPrompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\최인규\\AppData\\Local\\Temp\\ipykernel_18468\\515234645.py:1: RuntimeWarning: coroutine 'Extractor.generate_execution_plan.<locals>.apply_extract' was never awaited\n",
      "  len(SummaryExtractor().generate_execution_plan(load_kg))\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(SummaryExtractor().generate_execution_plan(load_kg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\최인규\\AppData\\Local\\Temp\\ipykernel_18468\\346516686.py:1: RuntimeWarning: coroutine 'Extractor.generate_execution_plan.<locals>.apply_extract' was never awaited\n",
      "  len(trans[2].generate_execution_plan(load_kg))\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trans[2].generate_execution_plan(load_kg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize the given text in less than 10 sentences.\n",
      "Please return the output in a JSON format that complies with the following schema as specified in JSON Schema:\n",
      "{\"properties\": {\"text\": {\"title\": \"Text\", \"type\": \"string\"}}, \"required\": [\"text\"], \"title\": \"StringIO\", \"type\": \"object\"}Do not use single quotes in your response but double quotes,properly escaped with a backslash.\n",
      "\n",
      "--------EXAMPLES-----------\n",
      "Example 1\n",
      "Input: {\n",
      "    \"text\": \"Artificial intelligence\\n\\nArtificial intelligence is transforming various industries by automating tasks that previously required human intelligence. From healthcare to finance, AI is being used to analyze vast amounts of data quickly and accurately. This technology is also driving innovations in areas like self-driving cars and personalized recommendations.\"\n",
      "}\n",
      "Output: {\n",
      "    \"text\": \"AI is revolutionizing industries by automating tasks, analyzing data, and driving innovations like self-driving cars and personalized recommendations.\"\n",
      "}\n",
      "-----------------------------\n",
      "\n",
      "Now perform the same with the following input\n",
      "Input: (None)\n",
      "Output: \n"
     ]
    }
   ],
   "source": [
    "print(SummaryExtractorPrompt().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying SummaryExtractor:   0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying SummaryExtractor:  48%|████▊     | 19/40 [00:06<00:04,  4.70it/s]Property 'summary' already exists in node 'e94830'. Skipping!\n",
      "Applying SummaryExtractor:  52%|█████▎    | 21/40 [00:07<00:05,  3.77it/s]Property 'summary' already exists in node '026ec0'. Skipping!\n",
      "Applying SummaryExtractor:  55%|█████▌    | 22/40 [00:07<00:04,  4.08it/s]Property 'summary' already exists in node 'e8e202'. Skipping!\n",
      "Applying SummaryExtractor:  60%|██████    | 24/40 [00:07<00:02,  5.43it/s]Property 'summary' already exists in node 'c07e55'. Skipping!\n",
      "Applying SummaryExtractor:  62%|██████▎   | 25/40 [00:08<00:05,  2.86it/s]Property 'summary' already exists in node '776df5'. Skipping!\n",
      "Applying SummaryExtractor:  65%|██████▌   | 26/40 [00:08<00:04,  3.17it/s]Property 'summary' already exists in node '0a5256'. Skipping!\n",
      "Applying SummaryExtractor:  68%|██████▊   | 27/40 [00:08<00:04,  3.20it/s]Property 'summary' already exists in node '438a11'. Skipping!\n",
      "Property 'summary' already exists in node 'dd6f45'. Skipping!\n",
      "Applying SummaryExtractor:  72%|███████▎  | 29/40 [00:09<00:02,  4.45it/s]Property 'summary' already exists in node '533e23'. Skipping!\n",
      "Applying SummaryExtractor:  75%|███████▌  | 30/40 [00:09<00:01,  5.07it/s]Property 'summary' already exists in node '666985'. Skipping!\n",
      "Applying SummaryExtractor:  78%|███████▊  | 31/40 [00:09<00:01,  5.06it/s]Property 'summary' already exists in node 'f6a9e5'. Skipping!\n",
      "Applying SummaryExtractor:  80%|████████  | 32/40 [00:09<00:01,  5.78it/s]Property 'summary' already exists in node 'd5f1ac'. Skipping!\n",
      "Applying SummaryExtractor:  85%|████████▌ | 34/40 [00:09<00:00,  6.75it/s]Property 'summary' already exists in node '6c5799'. Skipping!\n",
      "Applying SummaryExtractor:  88%|████████▊ | 35/40 [00:10<00:01,  3.79it/s]Property 'summary' already exists in node '717795'. Skipping!\n",
      "Applying SummaryExtractor:  90%|█████████ | 36/40 [00:10<00:01,  3.77it/s]Property 'summary' already exists in node '55746f'. Skipping!\n",
      "Property 'summary' already exists in node '761566'. Skipping!\n",
      "Applying SummaryExtractor:  98%|█████████▊| 39/40 [00:11<00:00,  3.76it/s]Property 'summary' already exists in node 'c60096'. Skipping!\n",
      "                                                                          \r"
     ]
    }
   ],
   "source": [
    "apply_transforms(load_kg, trans[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KnowledgeGraph(nodes: 55, relationships: 20)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_kg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_kg.save(\"test_kg/kg_SummaryExtractor.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_kg=KnowledgeGraph().load(\"test_kg/kg_SummaryExtractor.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KnowledgeGraph(nodes: 55, relationships: 20)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_kg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CustomNodeFilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomNodeFilter(name='CustomNodeFilter', filter_nodes=<function default_transforms.<locals>.<lambda> at 0x00000296DDF0E2A0>, llm=LangchainLLMWrapper(langchain_llm=ChatOpenAI(...)), scoring_prompt=QuestionPotentialPrompt(instruction=Given a document summary and node content, score the content of the node in 1 to 5 range., examples=[], language=english), min_score=2, rubrics={'score1_description': 'The page content is irrelevant or does not align with the main themes or topics of the document summary.', 'score2_description': \"The page content partially aligns with the document summary, but it includes unrelated details or lacks critical information related to the document's main themes.\", 'score3_description': 'The page content generally reflects the document summary but may miss key details or lack depth in addressing the main themes.', 'score4_description': 'The page content aligns well with the document summary, covering the main themes and topics with minor gaps or minimal unrelated information.', 'score5_description': \"The page content is highly relevant, accurate, and directly reflects the main themes of the document summary, covering all important details and adding depth to the understanding of the document's topics.\"})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.transforms.filters import CustomNodeFilter, QuestionPotentialInput, QuestionPotentialOutput, QuestionPotentialPrompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\최인규\\AppData\\Local\\Temp\\ipykernel_18468\\901010791.py:1: RuntimeWarning: coroutine 'NodeFilter.generate_execution_plan.<locals>.apply_filter' was never awaited\n",
      "  len(trans[3].generate_execution_plan(load_kg))\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trans[3].generate_execution_plan(load_kg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given a document summary and node content, score the content of the node in 1 to 5 range.\n",
      "Please return the output in a JSON format that complies with the following schema as specified in JSON Schema:\n",
      "{\"properties\": {\"score\": {\"description\": \"1 to 5 score\", \"title\": \"Score\", \"type\": \"integer\"}}, \"required\": [\"score\"], \"title\": \"QuestionPotentialOutput\", \"type\": \"object\"}Do not use single quotes in your response but double quotes,properly escaped with a backslash.\n",
      "\n",
      "-----------------------------\n",
      "\n",
      "Now perform the same with the following input\n",
      "input: {\n",
      "    \"document_summary\": \"\",\n",
      "    \"node_content\": \"\",\n",
      "    \"rubrics\": {\n",
      "        \"score1_description\": \"The page content is irrelevant or does not align with the main themes or topics of the document summary.\",\n",
      "        \"score2_description\": \"The page content partially aligns with the document summary, but it includes unrelated details or lacks critical information related to the document's main themes.\",\n",
      "        \"score3_description\": \"The page content generally reflects the document summary but may miss key details or lack depth in addressing the main themes.\",\n",
      "        \"score4_description\": \"The page content aligns well with the document summary, covering the main themes and topics with minor gaps or minimal unrelated information.\",\n",
      "        \"score5_description\": \"The page content is highly relevant, accurate, and directly reflects the main themes of the document summary, covering all important details and adding depth to the understanding of the document's topics.\"\n",
      "    }\n",
      "}\n",
      "Output: \n"
     ]
    }
   ],
   "source": [
    "print(trans[3].scoring_prompt.to_string(data = QuestionPotentialInput(document_summary=\"\", node_content=\"\", rubrics=trans[3].rubrics)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    }
   ],
   "source": [
    "apply_transforms(load_kg, trans[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KnowledgeGraph(nodes: 55, relationships: 20)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_kg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_kg.save(\"test_kg/kg_CustomNodeFilter.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_kg = KnowledgeGraph().load(\"test_kg/kg_CustomNodeFilter.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KnowledgeGraph(nodes: 55, relationships: 20)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_kg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parallel Transformation(Extractor)\n",
    "- EmbeddingExtractor\n",
    "- ThemesExtractor\n",
    "- NERExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ragas.testset.transforms.engine.Parallel at 0x296db34df10>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[EmbeddingExtractor(name='EmbeddingExtractor', filter_nodes=<function default_transforms.<locals>.<lambda> at 0x00000296E0B4D800>, property_name='summary_embedding', embed_property_name='summary', embedding_model=LangchainEmbeddingsWrapper(embeddings=OpenAIEmbeddings(...))),\n",
       " ThemesExtractor(name='ThemesExtractor', filter_nodes=<function default_transforms.<locals>.<lambda> at 0x00000296E0F8FF60>, llm=LangchainLLMWrapper(langchain_llm=ChatOpenAI(...)), merge_if_possible=True, max_token_limit=32000, tokenizer=<Encoding 'o200k_base'>, property_name='themes', prompt=ThemesAndConceptsExtractorPrompt(instruction=Extract the main themes and concepts from the given text., examples=[(TextWithExtractionLimit(text='Artificial intelligence is transforming industries by automating tasks requiring human intelligence. AI analyzes vast data quickly and accurately, driving innovations like self-driving cars and personalized recommendations.', max_num=10), ThemesAndConcepts(output=['Artificial intelligence', 'Automation', 'Data analysis', 'Innovation', 'Self-driving cars', 'Personalized recommendations']))], language=english), max_num_themes=10),\n",
       " NERExtractor(name='NERExtractor', filter_nodes=<function default_transforms.<locals>.<lambda> at 0x00000296E0B4E200>, llm=LangchainLLMWrapper(langchain_llm=ChatOpenAI(...)), merge_if_possible=True, max_token_limit=32000, tokenizer=<Encoding 'o200k_base'>, property_name='entities', prompt=NERPrompt(instruction=Extract the named entities from the given text, limiting the output to the top entities. Ensure the number of entities does not exceed the specified maximum., examples=[(TextWithExtractionLimit(text='Elon Musk, the CEO of Tesla and SpaceX, announced plans to expand operations to new locations in Europe and Asia.\\n                This expansion is expected to create thousands of jobs, particularly in cities like Berlin and Shanghai.', max_num=10), NEROutput(entities=['Elon Musk', 'Tesla', 'SpaceX', 'Europe', 'Asia', 'Berlin', 'Shanghai']))], language=english), max_num_entities=10)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans[4].transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\최인규\\AppData\\Local\\Temp\\ipykernel_18468\\229079015.py:1: RuntimeWarning: coroutine 'Extractor.generate_execution_plan.<locals>.apply_extract' was never awaited\n",
      "  len(trans[4].generate_execution_plan(load_kg))\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trans[4].generate_execution_plan(load_kg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.transforms.extractors.llm_based import ThemesExtractor, ThemesAndConceptsExtractorPrompt, NERExtractor, NERPrompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract the main themes and concepts from the given text.\n",
      "Please return the output in a JSON format that complies with the following schema as specified in JSON Schema:\n",
      "{\"properties\": {\"output\": {\"items\": {\"type\": \"string\"}, \"title\": \"Output\", \"type\": \"array\"}}, \"required\": [\"output\"], \"title\": \"ThemesAndConcepts\", \"type\": \"object\"}Do not use single quotes in your response but double quotes,properly escaped with a backslash.\n",
      "\n",
      "--------EXAMPLES-----------\n",
      "Example 1\n",
      "Input: {\n",
      "    \"text\": \"Artificial intelligence is transforming industries by automating tasks requiring human intelligence. AI analyzes vast data quickly and accurately, driving innovations like self-driving cars and personalized recommendations.\",\n",
      "    \"max_num\": 10\n",
      "}\n",
      "Output: {\n",
      "    \"output\": [\n",
      "        \"Artificial intelligence\",\n",
      "        \"Automation\",\n",
      "        \"Data analysis\",\n",
      "        \"Innovation\",\n",
      "        \"Self-driving cars\",\n",
      "        \"Personalized recommendations\"\n",
      "    ]\n",
      "}\n",
      "-----------------------------\n",
      "\n",
      "Now perform the same with the following input\n",
      "Input: (None)\n",
      "Output: \n"
     ]
    }
   ],
   "source": [
    "print(ThemesAndConceptsExtractorPrompt().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract the named entities from the given text, limiting the output to the top entities. Ensure the number of entities does not exceed the specified maximum.\n",
      "Please return the output in a JSON format that complies with the following schema as specified in JSON Schema:\n",
      "{\"properties\": {\"entities\": {\"items\": {\"type\": \"string\"}, \"title\": \"Entities\", \"type\": \"array\"}}, \"required\": [\"entities\"], \"title\": \"NEROutput\", \"type\": \"object\"}Do not use single quotes in your response but double quotes,properly escaped with a backslash.\n",
      "\n",
      "--------EXAMPLES-----------\n",
      "Example 1\n",
      "Input: {\n",
      "    \"text\": \"Elon Musk, the CEO of Tesla and SpaceX, announced plans to expand operations to new locations in Europe and Asia.\\n                This expansion is expected to create thousands of jobs, particularly in cities like Berlin and Shanghai.\",\n",
      "    \"max_num\": 10\n",
      "}\n",
      "Output: {\n",
      "    \"entities\": [\n",
      "        \"Elon Musk\",\n",
      "        \"Tesla\",\n",
      "        \"SpaceX\",\n",
      "        \"Europe\",\n",
      "        \"Asia\",\n",
      "        \"Berlin\",\n",
      "        \"Shanghai\"\n",
      "    ]\n",
      "}\n",
      "-----------------------------\n",
      "\n",
      "Now perform the same with the following input\n",
      "Input: (None)\n",
      "Output: \n"
     ]
    }
   ],
   "source": [
    "print(NERPrompt().to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\최인규\\AppData\\Local\\Temp\\ipykernel_18468\\2979926559.py:1: RuntimeWarning: coroutine 'Extractor.generate_execution_plan.<locals>.apply_extract' was never awaited\n",
      "  len(trans[4].transformations[0].generate_execution_plan(load_kg))\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trans[4].transformations[0].generate_execution_plan(load_kg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\최인규\\AppData\\Local\\Temp\\ipykernel_18468\\4038458144.py:1: RuntimeWarning: coroutine 'Extractor.generate_execution_plan.<locals>.apply_extract' was never awaited\n",
      "  len(trans[4].transformations[1].generate_execution_plan(load_kg))\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trans[4].transformations[1].generate_execution_plan(load_kg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\최인규\\AppData\\Local\\Temp\\ipykernel_18468\\3790453885.py:1: RuntimeWarning: coroutine 'Extractor.generate_execution_plan.<locals>.apply_extract' was never awaited\n",
      "  len(trans[4].transformations[2].generate_execution_plan(load_kg))\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trans[4].transformations[2].generate_execution_plan(load_kg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:   0%|          | 0/66 [00:00<?, ?it/s]Property 'summary_embedding' already exists in node '026ec0'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '55746f'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '533e23'. Skipping!\n",
      "Property 'summary_embedding' already exists in node 'e94830'. Skipping!\n",
      "Property 'summary_embedding' already exists in node 'e8e202'. Skipping!\n",
      "Property 'summary_embedding' already exists in node 'c60096'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '717795'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '438a11'. Skipping!\n",
      "Property 'summary_embedding' already exists in node 'dd6f45'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '776df5'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '6c5799'. Skipping!\n",
      "Property 'summary_embedding' already exists in node 'c07e55'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '0a5256'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '761566'. Skipping!\n",
      "Property 'summary_embedding' already exists in node 'f6a9e5'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '666985'. Skipping!\n",
      "Property 'summary_embedding' already exists in node 'd5f1ac'. Skipping!\n",
      "                                                                                                             \r"
     ]
    }
   ],
   "source": [
    "apply_transforms(load_kg, trans[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_kg.save(\"test_kg/kg_ParallelExtractor.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_kg = KnowledgeGraph().load(\"test_kg/kg_ParallelExtractor.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KnowledgeGraph(nodes: 55, relationships: 20)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_kg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parallel Transformation 2(Relation Builder)\n",
    "- CosineSimilarityBuilder\n",
    "- OverlapScoreBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ragas.testset.transforms.engine.Parallel at 0x296ddf2b9d0>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CosineSimilarityBuilder(name='CosineSimilarityBuilder', filter_nodes=<function default_transforms.<locals>.<lambda> at 0x00000296E2A93240>, property_name='summary_embedding', new_property_name='summary_similarity', threshold=0.7),\n",
       " OverlapScoreBuilder(name='', filter_nodes=<function default_transforms.<locals>.<lambda> at 0x00000296E2A93420>, property_name='entities', key_name=None, new_property_name='overlap_score', distance_measure=<DistanceMeasure.JARO_WINKLER: 'jaro_winkler'>, distance_threshold=0.9, threshold=0.01)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans[5].transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<coroutine object RelationshipBuilder.generate_execution_plan.<locals>.apply_build_relationships at 0x00000296DD90E420>,\n",
       " <coroutine object RelationshipBuilder.generate_execution_plan.<locals>.apply_build_relationships at 0x00000296DD90EB20>]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans[5].generate_execution_plan(load_kg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<coroutine object RelationshipBuilder.generate_execution_plan.<locals>.apply_build_relationships at 0x00000296DD90EC00>]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans[5].transformations[0].generate_execution_plan(load_kg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<coroutine object RelationshipBuilder.generate_execution_plan.<locals>.apply_build_relationships at 0x00000296DD90E6C0>]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans[5].transformations[1].generate_execution_plan(load_kg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    }
   ],
   "source": [
    "apply_transforms(load_kg, trans[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_kg.save(\"test_kg/kg_RelationshipBuilder.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_kg = KnowledgeGraph().load(\"test_kg/kg_RelationshipBuilder.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KnowledgeGraph(nodes: 55, relationships: 68)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_kg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KnowledgeGraph(nodes: 55, relationships: 68)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_kg = KnowledgeGraph.load(\"test_kg/kg_trans6.json\")\n",
    "loaded_kg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.persona import default_filter\n",
    "import numpy as np\n",
    "filter_fn = default_filter\n",
    "num_personas = 3\n",
    "nodes = [node for node in loaded_kg.nodes if filter_fn(node)]\n",
    "if len(nodes) == 0:\n",
    "    raise ValueError(\n",
    "        \"No nodes that satisfied the given filer. Try changing the filter.\"\n",
    "    )\n",
    "\n",
    "summaries = [node.properties.get(\"summary\") for node in nodes]\n",
    "summaries = [summary for summary in summaries if isinstance(summary, str)]\n",
    "num_personas = min(num_personas, len(summaries))\n",
    "\n",
    "embeddings = []\n",
    "for node in nodes:\n",
    "    embeddings.append(node.properties.get(\"summary_embedding\"))\n",
    "\n",
    "embeddings = np.array(embeddings)\n",
    "cosine_similarities = np.dot(embeddings, embeddings.T)\n",
    "\n",
    "groups = []\n",
    "visited = set()\n",
    "threshold = 0.75\n",
    "\n",
    "for i, _ in enumerate(summaries):\n",
    "    if i in visited:\n",
    "        continue\n",
    "    group = [i]\n",
    "    visited.add(i)\n",
    "    for j in range(i + 1, len(summaries)):\n",
    "        if cosine_similarities[i, j] > threshold:\n",
    "            group.append(j)\n",
    "            visited.add(j)\n",
    "    groups.append(group)\n",
    "\n",
    "top_summaries = []\n",
    "for group in groups:\n",
    "    representative_summary = max([summaries[i] for i in group], key=len)\n",
    "    top_summaries.append(representative_summary)\n",
    "\n",
    "if len(top_summaries) <= num_personas:\n",
    "    top_summaries.extend(\n",
    "        np.random.choice(top_summaries, num_personas - len(top_summaries))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Foundation models, particularly large language models (LLMs), are trained using diverse data sources such as books, websites, and public datasets. These models are evolving rapidly, with advancements in multi-modal capabilities that allow simultaneous learning from text and images. The training data is categorized into general and specialized datasets, enhancing the model's language modeling and generalization abilities. LLMs utilize unsupervised learning techniques to process vast amounts of data, which can include multilingual and scientific data. Fine-tuning and in-context learning are essential processes for adapting these models to specific tasks. The training of LLMs requires significant computational resources, often employing model parallelization to reduce training time. Pre-trained language models (PLMs) are developed using self-supervised or semi-supervised learning methods on large, unlabeled text corpora. Overall, foundation models are becoming foundational in AI, with ongoing research aimed at improving their capabilities.\""
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.persona import PersonaGenerationPrompt, generate_personas_from_kg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the provided summary, generate a single persona who would likely interact with or benefit from the content. Include a unique name and a concise role description of who they are.\n",
      "Please return the output in a JSON format that complies with the following schema as specified in JSON Schema:\n",
      "{\"properties\": {\"name\": {\"title\": \"Name\", \"type\": \"string\"}, \"role_description\": {\"title\": \"Role Description\", \"type\": \"string\"}}, \"required\": [\"name\", \"role_description\"], \"title\": \"Persona\", \"type\": \"object\"}Do not use single quotes in your response but double quotes,properly escaped with a backslash.\n",
      "\n",
      "--------EXAMPLES-----------\n",
      "Example 1\n",
      "Input: {\n",
      "    \"text\": \"Guide to Digital Marketing explains strategies for engaging audiences across various online platforms.\"\n",
      "}\n",
      "Output: {\n",
      "    \"name\": \"Digital Marketing Specialist\",\n",
      "    \"role_description\": \"Focuses on engaging audiences and growing the brand online.\"\n",
      "}\n",
      "-----------------------------\n",
      "\n",
      "Now perform the same with the following input\n",
      "Input: (None)\n",
      "Output: \n"
     ]
    }
   ],
   "source": [
    "print(PersonaGenerationPrompt().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating personas:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating personas: 100%|██████████| 20/20 [00:05<00:00,  3.79it/s]\n"
     ]
    }
   ],
   "source": [
    "persona_list=generate_personas_from_kg(\n",
    "    llm = generator_llm,\n",
    "    kg = loaded_kg,\n",
    "    num_personas=20\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Persona(name='Financial Technology Analyst', role_description='Analyzes the application of large language models in the finance sector, focusing on customer service automation and AI-driven solutions.'),\n",
       " Persona(name='AI Product Manager', role_description='Oversees the development of AI products, ensuring they leverage the latest advancements in language models and remain competitive in the market.'),\n",
       " Persona(name='AI Solutions Architect', role_description='Designs and implements AI models and solutions, focusing on performance and customer satisfaction.'),\n",
       " Persona(name='AI Research Scientist', role_description='Engages in research and development of foundation models, focusing on advancements in language processing and multi-modal learning.'),\n",
       " Persona(name='AI Research Engineer', role_description='Specializes in improving language models through fine-tuning and exploring efficient methods for domain-specific adaptations.'),\n",
       " Persona(name='AI Model Optimization Engineer', role_description=\"Specializes in enhancing large language models' performance through techniques like fine-tuning and data management.\"),\n",
       " Persona(name='Financial Data Scientist', role_description='Specializes in developing and optimizing LLMs tailored for the finance sector, focusing on data preprocessing and model tuning.'),\n",
       " Persona(name='Financial Data Analyst', role_description='Gathers and analyzes financial datasets to derive insights and support investment decisions.'),\n",
       " Persona(name='Financial Data Analyst', role_description='Specializes in analyzing financial news sentiment and optimizing product recommendations through tailored LLM models.'),\n",
       " Persona(name='Financial Data Scientist', role_description='Specializes in applying natural language processing techniques to analyze sentiment in financial news and optimize machine learning models for effective predictions.'),\n",
       " Persona(name='Machine Learning Engineer', role_description='Specializes in optimizing model performance through fine-tuning and careful selection of hyperparameters and training environments.'),\n",
       " Persona(name='Financial Data Scientist', role_description='Specializes in developing and fine-tuning algorithms for analyzing financial data while ensuring compliance with privacy and security regulations.'),\n",
       " Persona(name='Financial AI Specialist', role_description='Focuses on fine-tuning large language models for financial applications, evaluating model performance, and utilizing MLOps tools for efficient development and deployment.'),\n",
       " Persona(name='Financial Data Analyst', role_description='Specializes in evaluating and interpreting financial models to ensure high accuracy in predictions and reporting.'),\n",
       " Persona(name='Machine Learning Engineer', role_description='Responsible for optimizing and training models, specifically in the insurance domain, to ensure high-quality performance with user queries.'),\n",
       " Persona(name='Financial Technology Analyst', role_description='Specializes in integrating AI solutions to enhance operational efficiency in the finance sector.'),\n",
       " Persona(name='Financial Data Analyst', role_description='Specializes in analyzing financial data to improve prediction accuracy and customer service through advanced machine learning models.'),\n",
       " Persona(name='Insurance Technology Analyst', role_description='Researches and analyzes the application of AI innovations in the insurance industry to enhance customer experiences and optimize business processes.'),\n",
       " Persona(name='AI Research Analyst', role_description='Studies advancements in AI and evaluates the impact of large language models on various industries.'),\n",
       " Persona(name='Financial Data Scientist', role_description='Specializes in leveraging domain-specific language models to enhance applications of natural language processing in the financial sector.')]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "persona_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 시나리오 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.synthesizers import default_query_distribution\n",
    "query_distribution = default_query_distribution(generator_llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sigle Hop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_nodes = query_distribution[0][0].get_node_clusters(loaded_kg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 샘플사이즈를 고려해 각 노드에서 생성할 쿼리의 수를 계산\n",
    "int(np.ceil(10 / len(query_distribution[0][0].get_node_clusters(loaded_kg))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.synthesizers.prompts import ThemesPersonasMatchingPrompt, ThemesPersonasInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Foundation Model',\n",
       " 'LLM',\n",
       " 'Unsupervised learning',\n",
       " 'Supervised learning',\n",
       " 'Self-Supervised Learning',\n",
       " 'Semi-Supervised Learning',\n",
       " 'Wikipidia',\n",
       " 'Chowdhery et al.',\n",
       " 'Nijkamp et al.',\n",
       " 'Zhao et al.']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_nodes[0].get_property(\"entities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "persona_input = ThemesPersonasInput(themes=filtered_nodes[0].get_property(\"entities\"), personas=persona_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ThemesPersonasInput(themes=['Foundation Model', 'LLM', 'Unsupervised learning', 'Supervised learning', 'Self-Supervised Learning', 'Semi-Supervised Learning', 'Wikipidia', 'Chowdhery et al.', 'Nijkamp et al.', 'Zhao et al.'], personas=[Persona(name='Financial Technology Analyst', role_description='Analyzes the application of large language models in the finance sector, focusing on customer service automation and AI-driven solutions.'), Persona(name='AI Product Manager', role_description='Oversees the development of AI products, ensuring they leverage the latest advancements in language models and remain competitive in the market.'), Persona(name='AI Solutions Architect', role_description='Designs and implements AI models and solutions, focusing on performance and customer satisfaction.'), Persona(name='AI Research Scientist', role_description='Engages in research and development of foundation models, focusing on advancements in language processing and multi-modal learning.'), Persona(name='AI Research Engineer', role_description='Specializes in improving language models through fine-tuning and exploring efficient methods for domain-specific adaptations.'), Persona(name='AI Model Optimization Engineer', role_description=\"Specializes in enhancing large language models' performance through techniques like fine-tuning and data management.\"), Persona(name='Financial Data Scientist', role_description='Specializes in developing and optimizing LLMs tailored for the finance sector, focusing on data preprocessing and model tuning.'), Persona(name='Financial Data Analyst', role_description='Gathers and analyzes financial datasets to derive insights and support investment decisions.'), Persona(name='Financial Data Analyst', role_description='Specializes in analyzing financial news sentiment and optimizing product recommendations through tailored LLM models.'), Persona(name='Financial Data Scientist', role_description='Specializes in applying natural language processing techniques to analyze sentiment in financial news and optimize machine learning models for effective predictions.'), Persona(name='Machine Learning Engineer', role_description='Specializes in optimizing model performance through fine-tuning and careful selection of hyperparameters and training environments.'), Persona(name='Financial Data Scientist', role_description='Specializes in developing and fine-tuning algorithms for analyzing financial data while ensuring compliance with privacy and security regulations.'), Persona(name='Financial AI Specialist', role_description='Focuses on fine-tuning large language models for financial applications, evaluating model performance, and utilizing MLOps tools for efficient development and deployment.'), Persona(name='Financial Data Analyst', role_description='Specializes in evaluating and interpreting financial models to ensure high accuracy in predictions and reporting.'), Persona(name='Machine Learning Engineer', role_description='Responsible for optimizing and training models, specifically in the insurance domain, to ensure high-quality performance with user queries.'), Persona(name='Financial Technology Analyst', role_description='Specializes in integrating AI solutions to enhance operational efficiency in the finance sector.'), Persona(name='Financial Data Analyst', role_description='Specializes in analyzing financial data to improve prediction accuracy and customer service through advanced machine learning models.'), Persona(name='Insurance Technology Analyst', role_description='Researches and analyzes the application of AI innovations in the insurance industry to enhance customer experiences and optimize business processes.'), Persona(name='AI Research Analyst', role_description='Studies advancements in AI and evaluates the impact of large language models on various industries.'), Persona(name='Financial Data Scientist', role_description='Specializes in leveraging domain-specific language models to enhance applications of natural language processing in the financial sector.')])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "persona_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given a list of themes and personas with their roles, associate each persona with relevant themes based on their role description.\n",
      "Please return the output in a JSON format that complies with the following schema as specified in JSON Schema:\n",
      "{\"properties\": {\"mapping\": {\"additionalProperties\": {\"items\": {\"type\": \"string\"}, \"type\": \"array\"}, \"title\": \"Mapping\", \"type\": \"object\"}}, \"required\": [\"mapping\"], \"title\": \"PersonaThemesMapping\", \"type\": \"object\"}Do not use single quotes in your response but double quotes,properly escaped with a backslash.\n",
      "\n",
      "--------EXAMPLES-----------\n",
      "Example 1\n",
      "Input: {\n",
      "    \"themes\": [\n",
      "        \"Empathy\",\n",
      "        \"Inclusivity\",\n",
      "        \"Remote work\"\n",
      "    ],\n",
      "    \"personas\": [\n",
      "        {\n",
      "            \"name\": \"HR Manager\",\n",
      "            \"role_description\": \"Focuses on inclusivity and employee support.\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Remote Team Lead\",\n",
      "            \"role_description\": \"Manages remote team communication.\"\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "Output: {\n",
      "    \"mapping\": {\n",
      "        \"HR Manager\": [\n",
      "            \"Inclusivity\",\n",
      "            \"Empathy\"\n",
      "        ],\n",
      "        \"Remote Team Lead\": [\n",
      "            \"Remote work\",\n",
      "            \"Empathy\"\n",
      "        ]\n",
      "    }\n",
      "}\n",
      "-----------------------------\n",
      "\n",
      "Now perform the same with the following input\n",
      "input: {\n",
      "    \"themes\": [\n",
      "        \"Foundation Model\",\n",
      "        \"LLM\",\n",
      "        \"Unsupervised learning\",\n",
      "        \"Supervised learning\",\n",
      "        \"Self-Supervised Learning\",\n",
      "        \"Semi-Supervised Learning\",\n",
      "        \"Wikipidia\",\n",
      "        \"Chowdhery et al.\",\n",
      "        \"Nijkamp et al.\",\n",
      "        \"Zhao et al.\"\n",
      "    ],\n",
      "    \"personas\": [\n",
      "        {\n",
      "            \"name\": \"Financial Technology Analyst\",\n",
      "            \"role_description\": \"Analyzes the application of large language models in the finance sector, focusing on customer service automation and AI-driven solutions.\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"AI Product Manager\",\n",
      "            \"role_description\": \"Oversees the development of AI products, ensuring they leverage the latest advancements in language models and remain competitive in the market.\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"AI Solutions Architect\",\n",
      "            \"role_description\": \"Designs and implements AI models and solutions, focusing on performance and customer satisfaction.\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"AI Research Scientist\",\n",
      "            \"role_description\": \"Engages in research and development of foundation models, focusing on advancements in language processing and multi-modal learning.\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"AI Research Engineer\",\n",
      "            \"role_description\": \"Specializes in improving language models through fine-tuning and exploring efficient methods for domain-specific adaptations.\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"AI Model Optimization Engineer\",\n",
      "            \"role_description\": \"Specializes in enhancing large language models' performance through techniques like fine-tuning and data management.\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Financial Data Scientist\",\n",
      "            \"role_description\": \"Specializes in developing and optimizing LLMs tailored for the finance sector, focusing on data preprocessing and model tuning.\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Financial Data Analyst\",\n",
      "            \"role_description\": \"Gathers and analyzes financial datasets to derive insights and support investment decisions.\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Financial Data Analyst\",\n",
      "            \"role_description\": \"Specializes in analyzing financial news sentiment and optimizing product recommendations through tailored LLM models.\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Financial Data Scientist\",\n",
      "            \"role_description\": \"Specializes in applying natural language processing techniques to analyze sentiment in financial news and optimize machine learning models for effective predictions.\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Machine Learning Engineer\",\n",
      "            \"role_description\": \"Specializes in optimizing model performance through fine-tuning and careful selection of hyperparameters and training environments.\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Financial Data Scientist\",\n",
      "            \"role_description\": \"Specializes in developing and fine-tuning algorithms for analyzing financial data while ensuring compliance with privacy and security regulations.\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Financial AI Specialist\",\n",
      "            \"role_description\": \"Focuses on fine-tuning large language models for financial applications, evaluating model performance, and utilizing MLOps tools for efficient development and deployment.\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Financial Data Analyst\",\n",
      "            \"role_description\": \"Specializes in evaluating and interpreting financial models to ensure high accuracy in predictions and reporting.\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Machine Learning Engineer\",\n",
      "            \"role_description\": \"Responsible for optimizing and training models, specifically in the insurance domain, to ensure high-quality performance with user queries.\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Financial Technology Analyst\",\n",
      "            \"role_description\": \"Specializes in integrating AI solutions to enhance operational efficiency in the finance sector.\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Financial Data Analyst\",\n",
      "            \"role_description\": \"Specializes in analyzing financial data to improve prediction accuracy and customer service through advanced machine learning models.\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Insurance Technology Analyst\",\n",
      "            \"role_description\": \"Researches and analyzes the application of AI innovations in the insurance industry to enhance customer experiences and optimize business processes.\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"AI Research Analyst\",\n",
      "            \"role_description\": \"Studies advancements in AI and evaluates the impact of large language models on various industries.\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Financial Data Scientist\",\n",
      "            \"role_description\": \"Specializes in leveraging domain-specific language models to enhance applications of natural language processing in the financial sector.\"\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "Output: \n"
     ]
    }
   ],
   "source": [
    "print(ThemesPersonasMatchingPrompt().to_string(data=persona_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "persona_concepts = await ThemesPersonasMatchingPrompt().generate(data=persona_input, llm=generator_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Financial Technology Analyst': ['LLM',\n",
       "  'Foundation Model',\n",
       "  'Supervised learning'],\n",
       " 'AI Product Manager': ['Foundation Model', 'LLM'],\n",
       " 'AI Solutions Architect': ['Foundation Model', 'LLM'],\n",
       " 'AI Research Scientist': ['Foundation Model',\n",
       "  'Unsupervised learning',\n",
       "  'Self-Supervised Learning'],\n",
       " 'AI Research Engineer': ['LLM',\n",
       "  'Supervised learning',\n",
       "  'Self-Supervised Learning'],\n",
       " 'AI Model Optimization Engineer': ['LLM', 'Supervised learning'],\n",
       " 'Financial Data Scientist': ['LLM', 'Foundation Model'],\n",
       " 'Financial Data Analyst': ['LLM', 'Supervised learning'],\n",
       " 'Machine Learning Engineer': ['LLM', 'Supervised learning'],\n",
       " 'Financial AI Specialist': ['LLM', 'Supervised learning'],\n",
       " 'Insurance Technology Analyst': ['LLM', 'Foundation Model'],\n",
       " 'AI Research Analyst': ['Foundation Model', 'LLM']}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "persona_concepts.mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_scenarios  = query_distribution[0][0].prepare_combinations(\n",
    "                filtered_nodes[0],\n",
    "                filtered_nodes[0].get_property(\"entities\"),\n",
    "                personas=persona_list,\n",
    "                persona_concepts=persona_concepts.mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SingleHopScenario(\n",
       " nodes=1\n",
       " term=Supervised learning\n",
       " persona=name='AI Solutions Architect' role_description='Designs and implements AI models and solutions, focusing on performance and customer satisfaction.'\n",
       " style=QueryStyle.PERFECT_GRAMMAR\n",
       " length=QueryLength.SHORT)]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scenario_sample_list = query_distribution[0][0].sample_combinations(base_scenarios,1)\n",
    "scenario_sample_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_distribution[0][0].generate_sample(scenario=scenario_sample_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Node(id: 421d36, type: NodeType.CHUNK, properties: ['page_content', 'entities', 'themes'])]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scenario_sample_list[0].nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given a list of themes and personas with their roles, associate each persona with relevant themes based on their role description.\n",
      "Please return the output in a JSON format that complies with the following schema as specified in JSON Schema:\n",
      "{\"properties\": {\"mapping\": {\"additionalProperties\": {\"items\": {\"type\": \"string\"}, \"type\": \"array\"}, \"title\": \"Mapping\", \"type\": \"object\"}}, \"required\": [\"mapping\"], \"title\": \"PersonaThemesMapping\", \"type\": \"object\"}Do not use single quotes in your response but double quotes,properly escaped with a backslash.\n",
      "\n",
      "--------EXAMPLES-----------\n",
      "Example 1\n",
      "Input: {\n",
      "    \"themes\": [\n",
      "        \"Empathy\",\n",
      "        \"Inclusivity\",\n",
      "        \"Remote work\"\n",
      "    ],\n",
      "    \"personas\": [\n",
      "        {\n",
      "            \"name\": \"HR Manager\",\n",
      "            \"role_description\": \"Focuses on inclusivity and employee support.\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Remote Team Lead\",\n",
      "            \"role_description\": \"Manages remote team communication.\"\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "Output: {\n",
      "    \"mapping\": {\n",
      "        \"HR Manager\": [\n",
      "            \"Inclusivity\",\n",
      "            \"Empathy\"\n",
      "        ],\n",
      "        \"Remote Team Lead\": [\n",
      "            \"Remote work\",\n",
      "            \"Empathy\"\n",
      "        ]\n",
      "    }\n",
      "}\n",
      "-----------------------------\n",
      "\n",
      "Now perform the same with the following input\n",
      "Input: (None)\n",
      "Output: \n"
     ]
    }
   ],
   "source": [
    "print(ThemesPersonasMatchingPrompt().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(query_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(SingleHopSpecificQuerySynthesizer(name='single_hop_specifc_query_synthesizer', llm=LangchainLLMWrapper(langchain_llm=ChatOpenAI(...)), generate_query_reference_prompt=QueryAnswerGenerationPrompt(instruction=Generate a single-hop query and answer based on the specified conditions (persona, term, style, length) and the provided context. Ensure the answer is entirely faithful to the context, using only the information directly from the provided context.### Instructions:\n",
       "  1. **Generate a Query**: Based on the context, persona, term, style, and length, create a question that aligns with the persona's perspective and incorporates the term.\n",
       "  2. **Generate an Answer**: Using only the content from the provided context, construct a detailed answer to the query. Do not add any information not included in or inferable from the context.\n",
       "  , examples=[(QueryCondition(persona=Persona(name='Software Engineer', role_description='Focuses on coding best practices and system design.'), term='microservices', query_style='Formal', query_length='Medium', context='Microservices are an architectural style where applications are structured as a collection of loosely coupled services. Each service is fine-grained and focuses on a single functionality.'), GeneratedQueryAnswer(query='What is the purpose of microservices in software architecture?', answer='Microservices are designed to structure applications as a collection of loosely coupled services, each focusing on a single functionality.'))], language=english), theme_persona_matching_prompt=ThemesPersonasMatchingPrompt(instruction=Given a list of themes and personas with their roles, associate each persona with relevant themes based on their role description., examples=[(ThemesPersonasInput(themes=['Empathy', 'Inclusivity', 'Remote work'], personas=[Persona(name='HR Manager', role_description='Focuses on inclusivity and employee support.'), Persona(name='Remote Team Lead', role_description='Manages remote team communication.')]), PersonaThemesMapping(mapping={'HR Manager': ['Inclusivity', 'Empathy'], 'Remote Team Lead': ['Remote work', 'Empathy']}))], language=english), property_name='entities'),\n",
       "  0.3333333333333333),\n",
       " (MultiHopAbstractQuerySynthesizer(name='multi_hop_abstract_query_synthesizer', llm=LangchainLLMWrapper(langchain_llm=ChatOpenAI(...)), generate_query_reference_prompt=QueryAnswerGenerationPrompt(instruction=Generate a multi-hop query and answer based on the specified conditions (persona, themes, style, length) and the provided context. The themes represent a set of phrases either extracted or generated from the context, which highlight the suitability of the selected context for multi-hop query creation. Ensure the query explicitly incorporates these themes.### Instructions:\n",
       "  1. **Generate a Multi-Hop Query**: Use the provided context segments and themes to form a query that requires combining information from multiple segments (e.g., `<1-hop>` and `<2-hop>`). Ensure the query explicitly incorporates one or more themes and reflects their relevance to the context.\n",
       "  2. **Generate an Answer**: Use only the content from the provided context to create a detailed and faithful answer to the query. Avoid adding information that is not directly present or inferable from the given context.\n",
       "  3. **Multi-Hop Context Tags**:\n",
       "     - Each context segment is tagged as `<1-hop>`, `<2-hop>`, etc.\n",
       "     - Ensure the query uses information from at least two segments and connects them meaningfully., examples=[(QueryConditions(persona=Persona(name='Historian', role_description='Focuses on major scientific milestones and their global impact.'), themes=['Theory of Relativity', 'Experimental Validation'], query_style='Formal', query_length='Medium', context=['<1-hop> Albert Einstein developed the theory of relativity, introducing the concept of spacetime.', '<2-hop> The bending of light by gravity was confirmed during the 1919 solar eclipse, supporting Einstein’s theory.']), GeneratedQueryAnswer(query='How was the experimental validation of the theory of relativity achieved during the 1919 solar eclipse?', answer='The experimental validation of the theory of relativity was achieved during the 1919 solar eclipse by confirming the bending of light by gravity, which supported Einstein’s concept of spacetime as proposed in the theory.'))], language=english), concept_combination_prompt=ConceptCombinationPrompt(instruction=Form combinations by pairing concepts from at least two different lists.\n",
       "  **Instructions:**\n",
       "  - Review the concepts from each node.\n",
       "  - Identify concepts that can logically be connected or contrasted.\n",
       "  - Form combinations that involve concepts from different nodes.\n",
       "  - Each combination should include at least one concept from two or more nodes.\n",
       "  - List the combinations clearly and concisely.\n",
       "  - Do not repeat the same combination more than once., examples=[(ConceptsList(lists_of_concepts=[['Artificial intelligence', 'Automation'], ['Healthcare', 'Data privacy']], max_combinations=2), ConceptCombinations(combinations=[['Artificial intelligence', 'Healthcare'], ['Automation', 'Data privacy']]))], language=english), theme_persona_matching_prompt=ThemesPersonasMatchingPrompt(instruction=Given a list of themes and personas with their roles, associate each persona with relevant themes based on their role description., examples=[(ThemesPersonasInput(themes=['Empathy', 'Inclusivity', 'Remote work'], personas=[Persona(name='HR Manager', role_description='Focuses on inclusivity and employee support.'), Persona(name='Remote Team Lead', role_description='Manages remote team communication.')]), PersonaThemesMapping(mapping={'HR Manager': ['Inclusivity', 'Empathy'], 'Remote Team Lead': ['Remote work', 'Empathy']}))], language=english)),\n",
       "  0.3333333333333333),\n",
       " (MultiHopSpecificQuerySynthesizer(name='multi_hop_specific_query_synthesizer', llm=LangchainLLMWrapper(langchain_llm=ChatOpenAI(...)), generate_query_reference_prompt=QueryAnswerGenerationPrompt(instruction=Generate a multi-hop query and answer based on the specified conditions (persona, themes, style, length) and the provided context. The themes represent a set of phrases either extracted or generated from the context, which highlight the suitability of the selected context for multi-hop query creation. Ensure the query explicitly incorporates these themes.### Instructions:\n",
       "  1. **Generate a Multi-Hop Query**: Use the provided context segments and themes to form a query that requires combining information from multiple segments (e.g., `<1-hop>` and `<2-hop>`). Ensure the query explicitly incorporates one or more themes and reflects their relevance to the context.\n",
       "  2. **Generate an Answer**: Use only the content from the provided context to create a detailed and faithful answer to the query. Avoid adding information that is not directly present or inferable from the given context.\n",
       "  3. **Multi-Hop Context Tags**:\n",
       "     - Each context segment is tagged as `<1-hop>`, `<2-hop>`, etc.\n",
       "     - Ensure the query uses information from at least two segments and connects them meaningfully., examples=[(QueryConditions(persona=Persona(name='Historian', role_description='Focuses on major scientific milestones and their global impact.'), themes=['Theory of Relativity', 'Experimental Validation'], query_style='Formal', query_length='Medium', context=['<1-hop> Albert Einstein developed the theory of relativity, introducing the concept of spacetime.', '<2-hop> The bending of light by gravity was confirmed during the 1919 solar eclipse, supporting Einstein’s theory.']), GeneratedQueryAnswer(query='How was the experimental validation of the theory of relativity achieved during the 1919 solar eclipse?', answer='The experimental validation of the theory of relativity was achieved during the 1919 solar eclipse by confirming the bending of light by gravity, which supported Einstein’s concept of spacetime as proposed in the theory.'))], language=english), relation_type='entities_overlap', property_name='entities', theme_persona_matching_prompt=ThemesPersonasMatchingPrompt(instruction=Given a list of themes and personas with their roles, associate each persona with relevant themes based on their role description., examples=[(ThemesPersonasInput(themes=['Empathy', 'Inclusivity', 'Remote work'], personas=[Persona(name='HR Manager', role_description='Focuses on inclusivity and employee support.'), Persona(name='Remote Team Lead', role_description='Manages remote team communication.')]), PersonaThemesMapping(mapping={'HR Manager': ['Inclusivity', 'Empathy'], 'Remote Team Lead': ['Remote work', 'Empathy']}))], language=english)),\n",
       "  0.3333333333333333)]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.synthesizers.single_hop.specific import SingleHopSpecificQuerySynthesizer\n",
    "from ragas.testset.synthesizers.single_hop.prompts import QueryAnswerGenerationPrompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate a single-hop query and answer based on the specified conditions (persona, term, style, length) and the provided context. Ensure the answer is entirely faithful to the context, using only the information directly from the provided context.### Instructions:\n",
      "1. **Generate a Query**: Based on the context, persona, term, style, and length, create a question that aligns with the persona's perspective and incorporates the term.\n",
      "2. **Generate an Answer**: Using only the content from the provided context, construct a detailed answer to the query. Do not add any information not included in or inferable from the context.\n",
      "\n",
      "Please return the output in a JSON format that complies with the following schema as specified in JSON Schema:\n",
      "{\"properties\": {\"query\": {\"title\": \"Query\", \"type\": \"string\"}, \"answer\": {\"title\": \"Answer\", \"type\": \"string\"}}, \"required\": [\"query\", \"answer\"], \"title\": \"GeneratedQueryAnswer\", \"type\": \"object\"}Do not use single quotes in your response but double quotes,properly escaped with a backslash.\n",
      "\n",
      "--------EXAMPLES-----------\n",
      "Example 1\n",
      "Input: {\n",
      "    \"persona\": {\n",
      "        \"name\": \"Software Engineer\",\n",
      "        \"role_description\": \"Focuses on coding best practices and system design.\"\n",
      "    },\n",
      "    \"term\": \"microservices\",\n",
      "    \"query_style\": \"Formal\",\n",
      "    \"query_length\": \"Medium\",\n",
      "    \"context\": \"Microservices are an architectural style where applications are structured as a collection of loosely coupled services. Each service is fine-grained and focuses on a single functionality.\"\n",
      "}\n",
      "Output: {\n",
      "    \"query\": \"What is the purpose of microservices in software architecture?\",\n",
      "    \"answer\": \"Microservices are designed to structure applications as a collection of loosely coupled services, each focusing on a single functionality.\"\n",
      "}\n",
      "-----------------------------\n",
      "\n",
      "Now perform the same with the following input\n",
      "Input: (None)\n",
      "Output: \n"
     ]
    }
   ],
   "source": [
    "print(QueryAnswerGenerationPrompt().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = await query_distribution[0][0].generate_sample(scenario=scenario_sample_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How is supervised learning utilized in the training of foundation models?'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.user_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Supervised learning is used in the training of foundation models by learning from pre-processed text data.'"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.2. 파운데이션 모델(Foundation Model) 및 첫 번째 작업은 LLM이 학습될 리소스인 학습 데 LLM 사전학습(Pre-training) 이터 세트를 수집하는 것이다. 데이터는 책, 웹 생성형 AI 모델은 어떤 출력을 생성하는가에 사이트, 기사, 공개 데이터세트 등 다양한 소스에서 따라서 언어모델, 이미지 모델, 동영상 모델 등 가져올 수 있다. 유능한 LLM을 개발하기 위해 을 사용한다. 하지만 현재는 이미지와 텍스트를 사전 학습된 자료로 텍스트 데이터 세트를 사용 동시에 학습하는 멀티모달(Multi-modal) 모델들이 한다. 사전 학습된 코퍼스의 소스는 크게 일반 데이 하루가 다르게 성능과 기능이 업그레이드고 있고 터와 전문 데이터의 두 가지 유형으로 분류할 수 있으며 기초모델로 자리잡아가고 있다(정천수, 있으며 웹 페이지, 서적 및 대화 텍스트와 같은 2023d). 파운데이션 모델(Foundation Model)의 데 일반 데이터는 크고 다양하며 접근 가능한 특성 이터는 텍스트, 이미지, 음성, 정형데이터, 3D 시 으로 인해 대부분의 LLM에서 활용되며 LLM의 그널 등 구분하지 않고 학습에 이용되며 인간의 언어 모델링 및 일반화 능력을 향상시킬 수 있다. 창의력과 추론력을 포함한 일을 수행하며 이러한 또한 다국어 데이터, 과학 데이터 및 코드와 같은 기초모델은 방대한 양의 데이터를 비지도 학습 보다 전문화된 데이터 세트로 확장하여 LLM에 (Unsupervised learning)을 통해 모델을 학습시킨 특정 작업 해결 기능을 부여하는 연구도 발표되고 후 배포되어 사용자가 원하는 목적에 맞게 다운 있다(Chowdhery et al., 2023; Nijkamp et al., 2022). 스트림 작업에 대해 파인튜닝이나 문맥 내 학습 <그림 3>은 일반적인 LLM의 데이터 수집 및 (In-context learning)등과 같은 과정을 거처 완성 사전학습 절차를 보여주고 있다(Zhao et al., 2023). 되는 것이 파운데이션 모델이라고 볼 수 있다 모델 학습은 지도 학습(Supervised learning)을 사용 (Bommasani, et. al., 2021). 하여 전 처리된 텍스트 데이터에 대해 학습된다. 파운데이션 모델 중에서 언어모델로서의 LLM은 또한 모델과 데이터의 크기가 크기 때문에 모델을 일반 Text 데이터인 Wikipidia 등 거대한 일반적인 학습하려면 엄청난 계산 능력이 필요하며 학습 지식들을 수집하여 자기지도학습(Self-Supervised 시간을 줄이기 위해 모델 병렬화라는 기술이 사 Learning)이나 반자기지도학습(Semi-Supervised 용된다. 이렇게 대규모 언어 모델을 처음부터 학 Learning)을 사용하여 레이블링되지 않은 상당한 습하려면 상당한 투자가 필요하기 때문에 보다 양의 텍스트로 사전 학습된 언어 모델(Pre-trained 경제적인 대안으로 기존 언어 모델을 특정 사용 Language Model, PLM)이다.\n"
     ]
    }
   ],
   "source": [
    "print(sample.reference_contexts[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.synthesizers.single_hop.specific import (\n",
    "    SingleHopSpecificQuerySynthesizer,\n",
    ")\n",
    "\n",
    "distribution = [\n",
    "    (SingleHopSpecificQuerySynthesizer(llm=generator_llm), 1.0),\n",
    "]\n",
    "\n",
    "for query, _ in distribution:\n",
    "    prompts = await query.adapt_prompts(\"korean\", llm=generator_llm)\n",
    "    query.set_prompts(**prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset import TestsetGenerator\n",
    "\n",
    "generator = TestsetGenerator(llm=generator_llm, embedding_model=embedding_model, knowledge_graph=loaded_kg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SingleHopScenario(\n",
       " nodes=1\n",
       " term=\n",
       " persona=name='Financial Data Scientist' role_description='Specializes in analyzing financial data using advanced algorithms to ensure compliance, security, and accurate predictions.'\n",
       " style=QueryStyle.PERFECT_GRAMMAR\n",
       " length=QueryLength.MEDIUM),\n",
       " SingleHopScenario(\n",
       " nodes=1\n",
       " term=\n",
       " persona=name='Financial Data Scientist' role_description='Specializes in analyzing financial data using advanced algorithms to ensure compliance, security, and accurate predictions.'\n",
       " style=QueryStyle.PERFECT_GRAMMAR\n",
       " length=QueryLength.SHORT)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scenario_sample_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Node(id: 421d36, type: NodeType.CHUNK, properties: ['page_content', 'entities', 'themes']),\n",
       " Node(id: e4d209, type: NodeType.CHUNK, properties: ['page_content', 'entities', 'themes']),\n",
       " Node(id: 8cb85b, type: NodeType.CHUNK, properties: ['page_content', 'entities', 'themes']),\n",
       " Node(id: a4b129, type: NodeType.CHUNK, properties: ['page_content', 'themes', 'entities']),\n",
       " Node(id: 9d4361, type: NodeType.CHUNK, properties: ['page_content', 'themes', 'entities']),\n",
       " Node(id: 479c20, type: NodeType.CHUNK, properties: ['page_content', 'themes', 'entities']),\n",
       " Node(id: 3fd12d, type: NodeType.CHUNK, properties: ['page_content', 'themes', 'entities']),\n",
       " Node(id: e4b2f0, type: NodeType.CHUNK, properties: ['page_content', 'themes', 'entities']),\n",
       " Node(id: cab49c, type: NodeType.CHUNK, properties: ['page_content', 'themes', 'entities']),\n",
       " Node(id: 6b320f, type: NodeType.CHUNK, properties: ['page_content', 'themes', 'entities']),\n",
       " Node(id: 8eba31, type: NodeType.CHUNK, properties: ['page_content', 'themes', 'entities']),\n",
       " Node(id: d75d66, type: NodeType.CHUNK, properties: ['page_content', 'themes', 'entities']),\n",
       " Node(id: 6f7196, type: NodeType.CHUNK, properties: ['page_content', 'themes', 'entities'])]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[n for n in loaded_kg.nodes if n.type == NodeType.CHUNK]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Relationship(Node(id: a4b129) -> Node(id: e4b2f0), type: entities_overlap, properties: ['entities_overlap_score', 'overlapped_items']),\n",
       " Relationship(Node(id: a4b129) -> Node(id: d75d66), type: entities_overlap, properties: ['entities_overlap_score', 'overlapped_items']),\n",
       " Relationship(Node(id: 479c20) -> Node(id: e4b2f0), type: entities_overlap, properties: ['entities_overlap_score', 'overlapped_items']),\n",
       " Relationship(Node(id: e4b2f0) -> Node(id: cab49c), type: entities_overlap, properties: ['entities_overlap_score', 'overlapped_items'])]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[rel for rel in loaded_kg.relationships if rel.type == \"entities_overlap\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiHopAbstractQuerySynthesizer(name='multi_hop_abstract_query_synthesizer', llm=LangchainLLMWrapper(langchain_llm=ChatOpenAI(...)), generate_query_reference_prompt=QueryAnswerGenerationPrompt(instruction=Generate a multi-hop query and answer based on the specified conditions (persona, themes, style, length) and the provided context. The themes represent a set of phrases either extracted or generated from the context, which highlight the suitability of the selected context for multi-hop query creation. Ensure the query explicitly incorporates these themes.### Instructions:\n",
       "1. **Generate a Multi-Hop Query**: Use the provided context segments and themes to form a query that requires combining information from multiple segments (e.g., `<1-hop>` and `<2-hop>`). Ensure the query explicitly incorporates one or more themes and reflects their relevance to the context.\n",
       "2. **Generate an Answer**: Use only the content from the provided context to create a detailed and faithful answer to the query. Avoid adding information that is not directly present or inferable from the given context.\n",
       "3. **Multi-Hop Context Tags**:\n",
       "   - Each context segment is tagged as `<1-hop>`, `<2-hop>`, etc.\n",
       "   - Ensure the query uses information from at least two segments and connects them meaningfully., examples=[(QueryConditions(persona=Persona(name='Historian', role_description='Focuses on major scientific milestones and their global impact.'), themes=['Theory of Relativity', 'Experimental Validation'], query_style='Formal', query_length='Medium', context=['<1-hop> Albert Einstein developed the theory of relativity, introducing the concept of spacetime.', '<2-hop> The bending of light by gravity was confirmed during the 1919 solar eclipse, supporting Einstein’s theory.']), GeneratedQueryAnswer(query='How was the experimental validation of the theory of relativity achieved during the 1919 solar eclipse?', answer='The experimental validation of the theory of relativity was achieved during the 1919 solar eclipse by confirming the bending of light by gravity, which supported Einstein’s concept of spacetime as proposed in the theory.'))], language=english), concept_combination_prompt=ConceptCombinationPrompt(instruction=Form combinations by pairing concepts from at least two different lists.\n",
       "**Instructions:**\n",
       "- Review the concepts from each node.\n",
       "- Identify concepts that can logically be connected or contrasted.\n",
       "- Form combinations that involve concepts from different nodes.\n",
       "- Each combination should include at least one concept from two or more nodes.\n",
       "- List the combinations clearly and concisely.\n",
       "- Do not repeat the same combination more than once., examples=[(ConceptsList(lists_of_concepts=[['Artificial intelligence', 'Automation'], ['Healthcare', 'Data privacy']], max_combinations=2), ConceptCombinations(combinations=[['Artificial intelligence', 'Healthcare'], ['Automation', 'Data privacy']]))], language=english), theme_persona_matching_prompt=ThemesPersonasMatchingPrompt(instruction=Given a list of themes and personas with their roles, associate each persona with relevant themes based on their role description., examples=[(ThemesPersonasInput(themes=['Empathy', 'Inclusivity', 'Remote work'], personas=[Persona(name='HR Manager', role_description='Focuses on inclusivity and employee support.'), Persona(name='Remote Team Lead', role_description='Manages remote team communication.')]), PersonaThemesMapping(mapping={'HR Manager': ['Inclusivity', 'Empathy'], 'Remote Team Lead': ['Remote work', 'Empathy']}))], language=english))"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_distribution[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiHopSpecificQuerySynthesizer(name='multi_hop_specific_query_synthesizer', llm=LangchainLLMWrapper(langchain_llm=ChatOpenAI(...)), generate_query_reference_prompt=QueryAnswerGenerationPrompt(instruction=Generate a multi-hop query and answer based on the specified conditions (persona, themes, style, length) and the provided context. The themes represent a set of phrases either extracted or generated from the context, which highlight the suitability of the selected context for multi-hop query creation. Ensure the query explicitly incorporates these themes.### Instructions:\n",
       "1. **Generate a Multi-Hop Query**: Use the provided context segments and themes to form a query that requires combining information from multiple segments (e.g., `<1-hop>` and `<2-hop>`). Ensure the query explicitly incorporates one or more themes and reflects their relevance to the context.\n",
       "2. **Generate an Answer**: Use only the content from the provided context to create a detailed and faithful answer to the query. Avoid adding information that is not directly present or inferable from the given context.\n",
       "3. **Multi-Hop Context Tags**:\n",
       "   - Each context segment is tagged as `<1-hop>`, `<2-hop>`, etc.\n",
       "   - Ensure the query uses information from at least two segments and connects them meaningfully., examples=[(QueryConditions(persona=Persona(name='Historian', role_description='Focuses on major scientific milestones and their global impact.'), themes=['Theory of Relativity', 'Experimental Validation'], query_style='Formal', query_length='Medium', context=['<1-hop> Albert Einstein developed the theory of relativity, introducing the concept of spacetime.', '<2-hop> The bending of light by gravity was confirmed during the 1919 solar eclipse, supporting Einstein’s theory.']), GeneratedQueryAnswer(query='How was the experimental validation of the theory of relativity achieved during the 1919 solar eclipse?', answer='The experimental validation of the theory of relativity was achieved during the 1919 solar eclipse by confirming the bending of light by gravity, which supported Einstein’s concept of spacetime as proposed in the theory.'))], language=english), relation_type='entities_overlap', property_name='entities', theme_persona_matching_prompt=ThemesPersonasMatchingPrompt(instruction=Given a list of themes and personas with their roles, associate each persona with relevant themes based on their role description., examples=[(ThemesPersonasInput(themes=['Empathy', 'Inclusivity', 'Remote work'], personas=[Persona(name='HR Manager', role_description='Focuses on inclusivity and employee support.'), Persona(name='Remote Team Lead', role_description='Manages remote team communication.')]), PersonaThemesMapping(mapping={'HR Manager': ['Inclusivity', 'Empathy'], 'Remote Team Lead': ['Remote work', 'Empathy']}))], language=english))"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_distribution[2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplets = query_distribution[2][0].get_node_clusters(loaded_kg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(triplets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 시나리오 생성시 한글 적용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "for query, _ in query_distribution:\n",
    "    prompts = await query.adapt_prompts(\"korean\", llm=generator_llm)\n",
    "    query.set_prompts(**prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Scenarios: 100%|██████████| 3/3 [00:19<00:00,  6.62s/it]\n",
      "Generating Samples: 100%|██████████| 22/22 [00:07<00:00,  2.94it/s]\n"
     ]
    }
   ],
   "source": [
    "testset_generator = TestsetGenerator(embedding_model=ragas_embeddings, llm=generator_llm, knowledge_graph=loaded_kg, persona_list=persona_list)\n",
    "samples = testset_generator.generate(testset_size=20, query_distribution=query_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = samples.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference</th>\n",
       "      <th>synthesizer_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nijkamp et al.의 연구에서 파운데이션 모델의 특징은 무엇인가요?</td>\n",
       "      <td>[2.1.2. 파운데이션 모델(Foundation Model) 및 첫 번째 작업은 ...</td>\n",
       "      <td>Nijkamp et al.의 연구에 따르면, 파운데이션 모델은 일반 데이터와 전문 ...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023d에서 언급된 파인튜닝의 목적은 무엇인가요?</td>\n",
       "      <td>[사전학습하기 위한 사례에 맞게 파인튜닝을 하게 된다(정천수, 2023d). &lt;그림...</td>\n",
       "      <td>2023d에서는 사전학습하기 위한 사례에 맞게 파인튜닝을 하게 된다고 언급하고 있습니다.</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>금융 뉴스의 하이퍼파라미터 조정이 모델 성능에 미치는 영향은 무엇입니까?</td>\n",
       "      <td>[2) 모델의 용도: 모델의 용도에 따라 적합한 3.2.2. 하이퍼파라미터 조정 모...</td>\n",
       "      <td>금융 뉴스 하이퍼파라미터 조정은 모델의 성능을 극대화하기 위해 중요한 단계이며, 감...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BloombergGPT의 가용성에 대한 정보는 무엇인가요?</td>\n",
       "      <td>[모델의 가용성: 모델이 공개되어 있지 않다 적절히 조정함으로써 모델의 학습과 일반...</td>\n",
       "      <td>BloombergGPT는 금융 특화 대규모 언어 모델로, 100억 개의 매개변수를 ...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>금융 분야에서 LLM 파인튜닝을 수행할 때 고려해야 할 사항은 무엇인가요?</td>\n",
       "      <td>[3.3.2. 파인튜닝 알고리즘 적용 이터 보안 및 개인정보 보호를 고려하여 데이터...</td>\n",
       "      <td>금융 분야의 LLM 파인튜닝을 수행할 때는 데이터 보안, 개인정보 보호, 규정 준수...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>금융 분야에서 BloombergGPT의 역할은 무엇입니까?</td>\n",
       "      <td>[3.4. 금융 분야를 위한 LLM 구성 3.3.3. 보안 및 규정 준수 금융 분야...</td>\n",
       "      <td>BloombergGPT는 금융 분야에서 자연어 처리에 특화된 모델로, 금융 데이터의...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>금융 특화 파인튜닝이란 무엇이며, 이 기술이 금융 분야에서 어떻게 높은 성능을 발휘...</td>\n",
       "      <td>[하여, 금융 분야의 특정 작업에 높은 성능을 3.4.2. 금융 특화 파인튜닝 LL...</td>\n",
       "      <td>금융 특화 파인튜닝은 금융 분야의 특정 작업에 높은 성능을 보이는 LLM(대형 언어...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>금융 데이터의 특성을 고려한 파인튜닝이 고객 응대 개선에 어떻게 기여할 수 있나요?</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\n금융 데이터의 특성을 고려한 파인튜닝을 통해 된 데이터셋은 생...</td>\n",
       "      <td>금융 데이터의 특성을 고려한 파인튜닝은 LLM 모델의 성능을 향상시켜 금융 예측 및...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>금융 데이터의 특성을 고려한 LLM의 고객 응대 개선 가능성은 무엇인가요?</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\n금융 데이터의 특성을 고려한 파인튜닝을 통해 된 데이터셋은 생...</td>\n",
       "      <td>금융 데이터의 특성을 고려한 LLM은 고객 응대 개선에 있어 자동 응답 시스템과 금...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>금융 데이터의 특성을 고려한 LLM의 파인튜닝이 고객 응대 개선에 어떻게 기여할 수...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\n금융 데이터의 특성을 고려한 파인튜닝을 통해 된 데이터셋은 생...</td>\n",
       "      <td>금융 데이터의 특성을 고려한 LLM의 파인튜닝은 고객 응대 개선에 기여할 수 있는 ...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>금융 분야에서 모델 성능 극대화를 위해 하이퍼파라미터 조정은 어떻게 이루어지나요?</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\n2) 모델의 용도: 모델의 용도에 따라 적합한 3.2.2. 하...</td>\n",
       "      <td>금융 분야에서 모델 성능 극대화를 위해 하이퍼파라미터 조정은 모델의 용도에 따라 적...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>금융 분야에서 LLM 파인튜닝은 어떤 모델들이 사용되고 있나요?</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\n하여, 금융 분야의 특정 작업에 높은 성능을 3.4.2. 금융...</td>\n",
       "      <td>금융 분야에서 LLM 파인튜닝은 여러 모델들이 사용되고 있으며, 그 중 FinGPT...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>사전 훈련된 LLM 모델을 활용하여 금융 뉴스의 감성 분석을 수행할 때, 모델 성능...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\n2) 모델의 용도: 모델의 용도에 따라 적합한 3.2.2. 하...</td>\n",
       "      <td>금융 뉴스의 감성 분석을 수행하기 위해 사전 훈련된 LLM 모델을 활용할 때, 모델...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>금융 데이터의 특성을 고려한 모델 성능 극대화 방법은 무엇이며, 이를 위해 어떤 하...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\n2) 모델의 용도: 모델의 용도에 따라 적합한 3.2.2. 하...</td>\n",
       "      <td>금융 데이터의 특성을 고려한 모델 성능 극대화 방법으로는 파인튜닝을 통해 모델의 성...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>GPT-4와 FinGPT의 금융 도메인에서의 차이점은 무엇인가요?</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\n모델의 가용성: 모델이 공개되어 있지 않다 적절히 조정함으로써...</td>\n",
       "      <td>GPT-4는 OpenAI에서 개발한 대규모 언어 모델로, 1,750억 개 이상의 매...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>GPT-4와 FinGPT는 금융 분야에서 어떻게 다르게 활용되며, 각각의 모델이 가...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\n모델의 가용성: 모델이 공개되어 있지 않다 적절히 조정함으로써...</td>\n",
       "      <td>GPT-4는 OpenAI에서 개발한 대규모 언어 모델로, 1,750억 개 이상의 매...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>FinGPT는 금융 분야에서 어떤 방식으로 파인튜닝되어 있으며, BloombergG...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\n3.4. 금융 분야를 위한 LLM 구성 3.3.3. 보안 및 ...</td>\n",
       "      <td>FinGPT는 OpenAI의 GPT 아키텍처를 기반으로 하여 금융 도메인에서 처음부...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>FinGPT와 BloombergGPT는 금융 분야에서 어떻게 파인튜닝되어 있으며, ...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\n3.4. 금융 분야를 위한 LLM 구성 3.3.3. 보안 및 ...</td>\n",
       "      <td>FinGPT와 BloombergGPT는 금융 분야에 특화된 파인튜닝을 통해 높은 성...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>BLOOM과 BloombergGPT의 금융 특화 파인튜닝은 어떤 차이점이 있나요?</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\n&lt;표 7&gt; Evaluation Benchmarks of Blo...</td>\n",
       "      <td>BLOOM과 BloombergGPT는 모두 금융 분야에 특화된 LLM 모델이지만, ...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>BLOOM과 BloombergGPT는 금융 분야에서 어떤 역할을 하나요?</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\n&lt;표 7&gt; Evaluation Benchmarks of Blo...</td>\n",
       "      <td>BLOOM과 BloombergGPT는 금융 분야에서 각각의 특화된 역할을 수행합니다...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Mistral-7B SLM을 활용한 금융 데이터의 파인튜닝 방법은 무엇인가요?</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\n모델의 가용성: 모델이 공개되어 있지 않다 적절히 조정함으로써...</td>\n",
       "      <td>Mistral-7B SLM을 활용한 금융 데이터의 파인튜닝 방법은 금융 데이터의 특...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Mistral-7B SLM의 금융 분야에서의 활용 가능성과 이를 위한 데이터 전처리...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\n모델의 가용성: 모델이 공개되어 있지 않다 적절히 조정함으로써...</td>\n",
       "      <td>Mistral-7B SLM은 금융 데이터의 특성을 고려한 파인튜닝을 통해 모델의 성...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           user_input  \\\n",
       "0           Nijkamp et al.의 연구에서 파운데이션 모델의 특징은 무엇인가요?   \n",
       "1                        2023d에서 언급된 파인튜닝의 목적은 무엇인가요?   \n",
       "2            금융 뉴스의 하이퍼파라미터 조정이 모델 성능에 미치는 영향은 무엇입니까?   \n",
       "3                    BloombergGPT의 가용성에 대한 정보는 무엇인가요?   \n",
       "4           금융 분야에서 LLM 파인튜닝을 수행할 때 고려해야 할 사항은 무엇인가요?   \n",
       "5                    금융 분야에서 BloombergGPT의 역할은 무엇입니까?   \n",
       "6   금융 특화 파인튜닝이란 무엇이며, 이 기술이 금융 분야에서 어떻게 높은 성능을 발휘...   \n",
       "7      금융 데이터의 특성을 고려한 파인튜닝이 고객 응대 개선에 어떻게 기여할 수 있나요?   \n",
       "8           금융 데이터의 특성을 고려한 LLM의 고객 응대 개선 가능성은 무엇인가요?   \n",
       "9   금융 데이터의 특성을 고려한 LLM의 파인튜닝이 고객 응대 개선에 어떻게 기여할 수...   \n",
       "10      금융 분야에서 모델 성능 극대화를 위해 하이퍼파라미터 조정은 어떻게 이루어지나요?   \n",
       "11                금융 분야에서 LLM 파인튜닝은 어떤 모델들이 사용되고 있나요?   \n",
       "12  사전 훈련된 LLM 모델을 활용하여 금융 뉴스의 감성 분석을 수행할 때, 모델 성능...   \n",
       "13  금융 데이터의 특성을 고려한 모델 성능 극대화 방법은 무엇이며, 이를 위해 어떤 하...   \n",
       "14               GPT-4와 FinGPT의 금융 도메인에서의 차이점은 무엇인가요?   \n",
       "15  GPT-4와 FinGPT는 금융 분야에서 어떻게 다르게 활용되며, 각각의 모델이 가...   \n",
       "16  FinGPT는 금융 분야에서 어떤 방식으로 파인튜닝되어 있으며, BloombergG...   \n",
       "17  FinGPT와 BloombergGPT는 금융 분야에서 어떻게 파인튜닝되어 있으며, ...   \n",
       "18      BLOOM과 BloombergGPT의 금융 특화 파인튜닝은 어떤 차이점이 있나요?   \n",
       "19           BLOOM과 BloombergGPT는 금융 분야에서 어떤 역할을 하나요?   \n",
       "20        Mistral-7B SLM을 활용한 금융 데이터의 파인튜닝 방법은 무엇인가요?   \n",
       "21  Mistral-7B SLM의 금융 분야에서의 활용 가능성과 이를 위한 데이터 전처리...   \n",
       "\n",
       "                                   reference_contexts  \\\n",
       "0   [2.1.2. 파운데이션 모델(Foundation Model) 및 첫 번째 작업은 ...   \n",
       "1   [사전학습하기 위한 사례에 맞게 파인튜닝을 하게 된다(정천수, 2023d). <그림...   \n",
       "2   [2) 모델의 용도: 모델의 용도에 따라 적합한 3.2.2. 하이퍼파라미터 조정 모...   \n",
       "3   [모델의 가용성: 모델이 공개되어 있지 않다 적절히 조정함으로써 모델의 학습과 일반...   \n",
       "4   [3.3.2. 파인튜닝 알고리즘 적용 이터 보안 및 개인정보 보호를 고려하여 데이터...   \n",
       "5   [3.4. 금융 분야를 위한 LLM 구성 3.3.3. 보안 및 규정 준수 금융 분야...   \n",
       "6   [하여, 금융 분야의 특정 작업에 높은 성능을 3.4.2. 금융 특화 파인튜닝 LL...   \n",
       "7   [<1-hop>\\n\\n금융 데이터의 특성을 고려한 파인튜닝을 통해 된 데이터셋은 생...   \n",
       "8   [<1-hop>\\n\\n금융 데이터의 특성을 고려한 파인튜닝을 통해 된 데이터셋은 생...   \n",
       "9   [<1-hop>\\n\\n금융 데이터의 특성을 고려한 파인튜닝을 통해 된 데이터셋은 생...   \n",
       "10  [<1-hop>\\n\\n2) 모델의 용도: 모델의 용도에 따라 적합한 3.2.2. 하...   \n",
       "11  [<1-hop>\\n\\n하여, 금융 분야의 특정 작업에 높은 성능을 3.4.2. 금융...   \n",
       "12  [<1-hop>\\n\\n2) 모델의 용도: 모델의 용도에 따라 적합한 3.2.2. 하...   \n",
       "13  [<1-hop>\\n\\n2) 모델의 용도: 모델의 용도에 따라 적합한 3.2.2. 하...   \n",
       "14  [<1-hop>\\n\\n모델의 가용성: 모델이 공개되어 있지 않다 적절히 조정함으로써...   \n",
       "15  [<1-hop>\\n\\n모델의 가용성: 모델이 공개되어 있지 않다 적절히 조정함으로써...   \n",
       "16  [<1-hop>\\n\\n3.4. 금융 분야를 위한 LLM 구성 3.3.3. 보안 및 ...   \n",
       "17  [<1-hop>\\n\\n3.4. 금융 분야를 위한 LLM 구성 3.3.3. 보안 및 ...   \n",
       "18  [<1-hop>\\n\\n<표 7> Evaluation Benchmarks of Blo...   \n",
       "19  [<1-hop>\\n\\n<표 7> Evaluation Benchmarks of Blo...   \n",
       "20  [<1-hop>\\n\\n모델의 가용성: 모델이 공개되어 있지 않다 적절히 조정함으로써...   \n",
       "21  [<1-hop>\\n\\n모델의 가용성: 모델이 공개되어 있지 않다 적절히 조정함으로써...   \n",
       "\n",
       "                                            reference  \\\n",
       "0   Nijkamp et al.의 연구에 따르면, 파운데이션 모델은 일반 데이터와 전문 ...   \n",
       "1   2023d에서는 사전학습하기 위한 사례에 맞게 파인튜닝을 하게 된다고 언급하고 있습니다.   \n",
       "2   금융 뉴스 하이퍼파라미터 조정은 모델의 성능을 극대화하기 위해 중요한 단계이며, 감...   \n",
       "3   BloombergGPT는 금융 특화 대규모 언어 모델로, 100억 개의 매개변수를 ...   \n",
       "4   금융 분야의 LLM 파인튜닝을 수행할 때는 데이터 보안, 개인정보 보호, 규정 준수...   \n",
       "5   BloombergGPT는 금융 분야에서 자연어 처리에 특화된 모델로, 금융 데이터의...   \n",
       "6   금융 특화 파인튜닝은 금융 분야의 특정 작업에 높은 성능을 보이는 LLM(대형 언어...   \n",
       "7   금융 데이터의 특성을 고려한 파인튜닝은 LLM 모델의 성능을 향상시켜 금융 예측 및...   \n",
       "8   금융 데이터의 특성을 고려한 LLM은 고객 응대 개선에 있어 자동 응답 시스템과 금...   \n",
       "9   금융 데이터의 특성을 고려한 LLM의 파인튜닝은 고객 응대 개선에 기여할 수 있는 ...   \n",
       "10  금융 분야에서 모델 성능 극대화를 위해 하이퍼파라미터 조정은 모델의 용도에 따라 적...   \n",
       "11  금융 분야에서 LLM 파인튜닝은 여러 모델들이 사용되고 있으며, 그 중 FinGPT...   \n",
       "12  금융 뉴스의 감성 분석을 수행하기 위해 사전 훈련된 LLM 모델을 활용할 때, 모델...   \n",
       "13  금융 데이터의 특성을 고려한 모델 성능 극대화 방법으로는 파인튜닝을 통해 모델의 성...   \n",
       "14  GPT-4는 OpenAI에서 개발한 대규모 언어 모델로, 1,750억 개 이상의 매...   \n",
       "15  GPT-4는 OpenAI에서 개발한 대규모 언어 모델로, 1,750억 개 이상의 매...   \n",
       "16  FinGPT는 OpenAI의 GPT 아키텍처를 기반으로 하여 금융 도메인에서 처음부...   \n",
       "17  FinGPT와 BloombergGPT는 금융 분야에 특화된 파인튜닝을 통해 높은 성...   \n",
       "18  BLOOM과 BloombergGPT는 모두 금융 분야에 특화된 LLM 모델이지만, ...   \n",
       "19  BLOOM과 BloombergGPT는 금융 분야에서 각각의 특화된 역할을 수행합니다...   \n",
       "20  Mistral-7B SLM을 활용한 금융 데이터의 파인튜닝 방법은 금융 데이터의 특...   \n",
       "21  Mistral-7B SLM은 금융 데이터의 특성을 고려한 파인튜닝을 통해 모델의 성...   \n",
       "\n",
       "                        synthesizer_name  \n",
       "0   single_hop_specifc_query_synthesizer  \n",
       "1   single_hop_specifc_query_synthesizer  \n",
       "2   single_hop_specifc_query_synthesizer  \n",
       "3   single_hop_specifc_query_synthesizer  \n",
       "4   single_hop_specifc_query_synthesizer  \n",
       "5   single_hop_specifc_query_synthesizer  \n",
       "6   single_hop_specifc_query_synthesizer  \n",
       "7   multi_hop_abstract_query_synthesizer  \n",
       "8   multi_hop_abstract_query_synthesizer  \n",
       "9   multi_hop_abstract_query_synthesizer  \n",
       "10  multi_hop_abstract_query_synthesizer  \n",
       "11  multi_hop_abstract_query_synthesizer  \n",
       "12  multi_hop_abstract_query_synthesizer  \n",
       "13  multi_hop_abstract_query_synthesizer  \n",
       "14  multi_hop_specific_query_synthesizer  \n",
       "15  multi_hop_specific_query_synthesizer  \n",
       "16  multi_hop_specific_query_synthesizer  \n",
       "17  multi_hop_specific_query_synthesizer  \n",
       "18  multi_hop_specific_query_synthesizer  \n",
       "19  multi_hop_specific_query_synthesizer  \n",
       "20  multi_hop_specific_query_synthesizer  \n",
       "21  multi_hop_specific_query_synthesizer  "
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What role does Semi-Supervised Learning play in the development of foundation models?\n",
      "What does the term '2023d' refer to in the context of fine-tuning for pre-training?\n",
      "금융 특화 LLM의 하이퍼파라미터 조정은 왜 중요한가?\n",
      "Mistral-7B는 어떤 모델이고, 금융 분야에서 어떻게 사용될 수 있나요?\n",
      "금융 데이터의 특성을 고려한 파인튜닝이 고객 응대 개선에 어떻게 기여할 수 있는지 설명해줘.\n",
      "What are the implications of fine-tuning LLMs for financial data in improving customer service and predictive accuracy?\n",
      "What are the implications of fine-tuning LLMs for financial data in improving customer service and predictive accuracy?\n",
      "금융 분야 특화 모델인 BloombergGPT와 FinBERT의 성능 극대화에 필요한 하이퍼파라미터 조정은 어떻게 이루어져야 하나요?\n",
      "GPT-4와 FinGPT는 금융 분야에서 어떻게 활용되고 있으며, 이 두 모델의 차이점은 무엇인가요?\n",
      "FinGPT와 BloombergGPT는 금융 분야에서 어떻게 특화된 파인튜닝을 통해 성능을 향상시키고 있나요?\n",
      "What advancements have been made in financial language models like BloombergGPT and how do they compare to other models such as FinGPT and BLOOM in terms of their training and application in the finance sector?\n",
      "Mistral-7B SLM은 금융 데이터의 특성을 고려한 파인튜닝을 통해 어떤 성능 향상을 이루었나요?\n"
     ]
    }
   ],
   "source": [
    "for q in df.user_input:\n",
    "    print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference</th>\n",
       "      <th>synthesizer_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What role does Semi-Supervised Learning play i...</td>\n",
       "      <td>[2.1.2. 파운데이션 모델(Foundation Model) 및 첫 번째 작업은 ...</td>\n",
       "      <td>Semi-Supervised Learning is utilized in the de...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What does the term '2023d' refer to in the con...</td>\n",
       "      <td>[사전학습하기 위한 사례에 맞게 파인튜닝을 하게 된다(정천수, 2023d). &lt;그림...</td>\n",
       "      <td>'2023d' refers to a source by Jeong Cheon-soo ...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>금융 특화 LLM의 하이퍼파라미터 조정은 왜 중요한가?</td>\n",
       "      <td>[2) 모델의 용도: 모델의 용도에 따라 적합한 3.2.2. 하이퍼파라미터 조정 모...</td>\n",
       "      <td>금융 특화 LLM의 하이퍼파라미터 조정은 모델의 성능을 극대화하기 위해 중요한 단계...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mistral-7B는 어떤 모델이고, 금융 분야에서 어떻게 사용될 수 있나요?</td>\n",
       "      <td>[모델의 가용성: 모델이 공개되어 있지 않다 적절히 조정함으로써 모델의 학습과 일반...</td>\n",
       "      <td>Mistral-7B는 프랑스의 스타트업 미스트랄 AI가 개발한 매개변수 73억 개의...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>금융 데이터의 특성을 고려한 파인튜닝이 고객 응대 개선에 어떻게 기여할 수 있는지 ...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\n금융 데이터의 특성을 고려한 파인튜닝을 통해 된 데이터셋은 생...</td>\n",
       "      <td>금융 데이터의 특성을 고려한 파인튜닝은 고객 응대 개선에 기여할 수 있는 여러 방법...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What are the implications of fine-tuning LLMs ...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\n금융 데이터의 특성을 고려한 파인튜닝을 통해 된 데이터셋은 생...</td>\n",
       "      <td>Fine-tuning LLMs for financial data has signif...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What are the implications of fine-tuning LLMs ...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\n금융 데이터의 특성을 고려한 파인튜닝을 통해 된 데이터셋은 생...</td>\n",
       "      <td>Fine-tuning LLMs for financial data has signif...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>금융 분야 특화 모델인 BloombergGPT와 FinBERT의 성능 극대화에 필요...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\n2) 모델의 용도: 모델의 용도에 따라 적합한 3.2.2. 하...</td>\n",
       "      <td>금융 분야 특화 모델인 BloombergGPT와 FinBERT의 성능 극대화를 위해...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>GPT-4와 FinGPT는 금융 분야에서 어떻게 활용되고 있으며, 이 두 모델의 차...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\n모델의 가용성: 모델이 공개되어 있지 않다 적절히 조정함으로써...</td>\n",
       "      <td>GPT-4는 OpenAI에서 개발한 대규모 언어 모델로, 1,750억 개 이상의 매...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>FinGPT와 BloombergGPT는 금융 분야에서 어떻게 특화된 파인튜닝을 통해...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\n3.4. 금융 분야를 위한 LLM 구성 3.3.3. 보안 및 ...</td>\n",
       "      <td>FinGPT와 BloombergGPT는 금융 분야에 특화된 파인튜닝을 통해 성능을 ...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>What advancements have been made in financial ...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\n&lt;표 7&gt; Evaluation Benchmarks of Blo...</td>\n",
       "      <td>BloombergGPT has shown outstanding performance...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Mistral-7B SLM은 금융 데이터의 특성을 고려한 파인튜닝을 통해 어떤 성능...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\n모델의 가용성: 모델이 공개되어 있지 않다 적절히 조정함으로써...</td>\n",
       "      <td>Mistral-7B SLM은 금융 데이터의 특성을 고려한 파인튜닝을 통해 모델의 성...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           user_input  \\\n",
       "0   What role does Semi-Supervised Learning play i...   \n",
       "1   What does the term '2023d' refer to in the con...   \n",
       "2                      금융 특화 LLM의 하이퍼파라미터 조정은 왜 중요한가?   \n",
       "3         Mistral-7B는 어떤 모델이고, 금융 분야에서 어떻게 사용될 수 있나요?   \n",
       "4   금융 데이터의 특성을 고려한 파인튜닝이 고객 응대 개선에 어떻게 기여할 수 있는지 ...   \n",
       "5   What are the implications of fine-tuning LLMs ...   \n",
       "6   What are the implications of fine-tuning LLMs ...   \n",
       "7   금융 분야 특화 모델인 BloombergGPT와 FinBERT의 성능 극대화에 필요...   \n",
       "8   GPT-4와 FinGPT는 금융 분야에서 어떻게 활용되고 있으며, 이 두 모델의 차...   \n",
       "9   FinGPT와 BloombergGPT는 금융 분야에서 어떻게 특화된 파인튜닝을 통해...   \n",
       "10  What advancements have been made in financial ...   \n",
       "11  Mistral-7B SLM은 금융 데이터의 특성을 고려한 파인튜닝을 통해 어떤 성능...   \n",
       "\n",
       "                                   reference_contexts  \\\n",
       "0   [2.1.2. 파운데이션 모델(Foundation Model) 및 첫 번째 작업은 ...   \n",
       "1   [사전학습하기 위한 사례에 맞게 파인튜닝을 하게 된다(정천수, 2023d). <그림...   \n",
       "2   [2) 모델의 용도: 모델의 용도에 따라 적합한 3.2.2. 하이퍼파라미터 조정 모...   \n",
       "3   [모델의 가용성: 모델이 공개되어 있지 않다 적절히 조정함으로써 모델의 학습과 일반...   \n",
       "4   [<1-hop>\\n\\n금융 데이터의 특성을 고려한 파인튜닝을 통해 된 데이터셋은 생...   \n",
       "5   [<1-hop>\\n\\n금융 데이터의 특성을 고려한 파인튜닝을 통해 된 데이터셋은 생...   \n",
       "6   [<1-hop>\\n\\n금융 데이터의 특성을 고려한 파인튜닝을 통해 된 데이터셋은 생...   \n",
       "7   [<1-hop>\\n\\n2) 모델의 용도: 모델의 용도에 따라 적합한 3.2.2. 하...   \n",
       "8   [<1-hop>\\n\\n모델의 가용성: 모델이 공개되어 있지 않다 적절히 조정함으로써...   \n",
       "9   [<1-hop>\\n\\n3.4. 금융 분야를 위한 LLM 구성 3.3.3. 보안 및 ...   \n",
       "10  [<1-hop>\\n\\n<표 7> Evaluation Benchmarks of Blo...   \n",
       "11  [<1-hop>\\n\\n모델의 가용성: 모델이 공개되어 있지 않다 적절히 조정함으로써...   \n",
       "\n",
       "                                            reference  \\\n",
       "0   Semi-Supervised Learning is utilized in the de...   \n",
       "1   '2023d' refers to a source by Jeong Cheon-soo ...   \n",
       "2   금융 특화 LLM의 하이퍼파라미터 조정은 모델의 성능을 극대화하기 위해 중요한 단계...   \n",
       "3   Mistral-7B는 프랑스의 스타트업 미스트랄 AI가 개발한 매개변수 73억 개의...   \n",
       "4   금융 데이터의 특성을 고려한 파인튜닝은 고객 응대 개선에 기여할 수 있는 여러 방법...   \n",
       "5   Fine-tuning LLMs for financial data has signif...   \n",
       "6   Fine-tuning LLMs for financial data has signif...   \n",
       "7   금융 분야 특화 모델인 BloombergGPT와 FinBERT의 성능 극대화를 위해...   \n",
       "8   GPT-4는 OpenAI에서 개발한 대규모 언어 모델로, 1,750억 개 이상의 매...   \n",
       "9   FinGPT와 BloombergGPT는 금융 분야에 특화된 파인튜닝을 통해 성능을 ...   \n",
       "10  BloombergGPT has shown outstanding performance...   \n",
       "11  Mistral-7B SLM은 금융 데이터의 특성을 고려한 파인튜닝을 통해 모델의 성...   \n",
       "\n",
       "                        synthesizer_name  \n",
       "0   single_hop_specifc_query_synthesizer  \n",
       "1   single_hop_specifc_query_synthesizer  \n",
       "2   single_hop_specifc_query_synthesizer  \n",
       "3   single_hop_specifc_query_synthesizer  \n",
       "4   multi_hop_abstract_query_synthesizer  \n",
       "5   multi_hop_abstract_query_synthesizer  \n",
       "6   multi_hop_abstract_query_synthesizer  \n",
       "7   multi_hop_abstract_query_synthesizer  \n",
       "8   multi_hop_specific_query_synthesizer  \n",
       "9   multi_hop_specific_query_synthesizer  \n",
       "10  multi_hop_specific_query_synthesizer  \n",
       "11  multi_hop_specific_query_synthesizer  "
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 중복 Node 제거 테스트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. HeadlineSpliter 적용이후"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_kg=KnowledgeGraph().load(\"test_kg/kg_HeadlineSpliter.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KnowledgeGraph(nodes: 55, relationships: 20)"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_kg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_nodes = set()\n",
    "for node in loaded_kg.nodes:\n",
    "    unique_nodes.add(node)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_rels = set()\n",
    "for rel in loaded_kg.relationships:\n",
    "    unique_rels.add(rel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_rels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rel in loaded_kg.relationships:\n",
    "    if (rel.source in unique_nodes) & (rel.target in unique_nodes):\n",
    "        continue\n",
    "    else:\n",
    "        print(rel.source, rel.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.transforms.base import NodeFilter\n",
    "# from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_node = loaded_kg.nodes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 25],\n",
       " [1],\n",
       " [2, 26],\n",
       " [3, 27],\n",
       " [4],\n",
       " [5, 30],\n",
       " [6, 31],\n",
       " [7, 32],\n",
       " [8, 33],\n",
       " [9, 34],\n",
       " [10],\n",
       " [11, 37],\n",
       " [12, 38],\n",
       " [13],\n",
       " [14],\n",
       " [15, 44],\n",
       " [16, 45],\n",
       " [17],\n",
       " [18, 46],\n",
       " [19, 47],\n",
       " [20],\n",
       " [21],\n",
       " [22, 52],\n",
       " [23, 53],\n",
       " [24, 54],\n",
       " [0, 25],\n",
       " [2, 26],\n",
       " [3, 27],\n",
       " [28],\n",
       " [29],\n",
       " [5, 30],\n",
       " [6, 31],\n",
       " [7, 32],\n",
       " [8, 33],\n",
       " [9, 34],\n",
       " [35],\n",
       " [36],\n",
       " [11, 37],\n",
       " [12, 38],\n",
       " [39],\n",
       " [40],\n",
       " [41],\n",
       " [42],\n",
       " [43],\n",
       " [15, 44],\n",
       " [16, 45],\n",
       " [18, 46],\n",
       " [19, 47],\n",
       " [48],\n",
       " [49],\n",
       " [50],\n",
       " [51],\n",
       " [22, 52],\n",
       " [23, 53],\n",
       " [24, 54]]"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[idx for idx, node in enumerate(loaded_kg.nodes) if node == check_node] for check_node in loaded_kg.nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuplicateFilter(NodeFilter):\n",
    "    async def custom_filter(self, node, kg):\n",
    "        return len([n for n in kg.nodes if n==node])>1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\최인규\\AppData\\Local\\Temp\\ipykernel_14412\\1716648303.py:1: RuntimeWarning: coroutine 'NodeFilter.generate_execution_plan.<locals>.apply_filter' was never awaited\n",
      "  len(DuplicateFilter().generate_execution_plan(loaded_kg))\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(DuplicateFilter().generate_execution_plan(loaded_kg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KnowledgeGraph(nodes: 55, relationships: 20)"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_kg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "test_kg = deepcopy(loaded_kg)\n",
    "apply_transforms(test_kg, DuplicateFilter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[n for n in loaded_kg.nodes if n not in test_kg.nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_kg = KnowledgeGraph(nodes=list(unique_nodes), relationships=loaded_kg.relationships)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_kg.save(\"test_kg/kg_HeadlineSpliter_NoDupli.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_kg=KnowledgeGraph().load(\"test_kg/kg_HeadlineSpliter_nodupli.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KnowledgeGraph(nodes: 38, relationships: 20)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_kg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이후 변환 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying SummaryExtractor:   0%|          | 0/23 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                             \r"
     ]
    }
   ],
   "source": [
    "apply_transforms(new_kg,trans[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KnowledgeGraph(nodes: 38, relationships: 32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_kg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Node(id: d5f1ac, type: NodeType.DOCUMENT, properties: ['page_content', 'document_metadata', 'headlines', 'summary', 'summary_embedding']),\n",
       " Node(id: dd6f45, type: NodeType.DOCUMENT, properties: ['page_content', 'document_metadata', 'headlines', 'summary', 'summary_embedding']),\n",
       " Node(id: 717795, type: NodeType.DOCUMENT, properties: ['page_content', 'document_metadata', 'headlines', 'summary', 'summary_embedding']),\n",
       " Node(id: e8e202, type: NodeType.DOCUMENT, properties: ['page_content', 'document_metadata', 'headlines', 'summary', 'summary_embedding']),\n",
       " Node(id: cab49c, type: NodeType.CHUNK, properties: ['page_content', 'entities', 'themes']),\n",
       " Node(id: 6c5799, type: NodeType.DOCUMENT, properties: ['page_content', 'document_metadata', 'headlines', 'summary', 'summary_embedding']),\n",
       " Node(id: c07e55, type: NodeType.DOCUMENT, properties: ['page_content', 'document_metadata', 'headlines', 'summary', 'summary_embedding']),\n",
       " Node(id: 3fd12d, type: NodeType.CHUNK, properties: ['page_content', 'entities', 'themes']),\n",
       " Node(id: d75d66, type: NodeType.CHUNK, properties: ['page_content', 'themes', 'entities']),\n",
       " Node(id: 533e23, type: NodeType.DOCUMENT, properties: ['page_content', 'document_metadata', 'headlines', 'summary', 'summary_embedding']),\n",
       " Node(id: 542663, type: NodeType.DOCUMENT, properties: ['page_content', 'document_metadata', 'headlines', 'summary', 'summary_embedding']),\n",
       " Node(id: 479c20, type: NodeType.CHUNK, properties: ['page_content', 'themes', 'entities']),\n",
       " Node(id: 761566, type: NodeType.DOCUMENT, properties: ['page_content', 'document_metadata', 'headlines', 'summary', 'summary_embedding']),\n",
       " Node(id: 6b320f, type: NodeType.CHUNK, properties: ['page_content', 'themes', 'entities']),\n",
       " Node(id: f63445, type: NodeType.DOCUMENT, properties: ['page_content', 'document_metadata', 'headlines', 'summary', 'summary_embedding']),\n",
       " Node(id: e4d209, type: NodeType.CHUNK, properties: ['page_content', 'themes', 'entities']),\n",
       " Node(id: e94830, type: NodeType.DOCUMENT, properties: ['page_content', 'document_metadata', 'headlines', 'summary', 'summary_embedding']),\n",
       " Node(id: 6f7196, type: NodeType.CHUNK, properties: ['page_content', 'themes', 'entities']),\n",
       " Node(id: 438a11, type: NodeType.DOCUMENT, properties: ['page_content', 'document_metadata', 'headlines', 'summary', 'summary_embedding']),\n",
       " Node(id: 026ec0, type: NodeType.DOCUMENT, properties: ['page_content', 'document_metadata', 'headlines', 'summary', 'summary_embedding']),\n",
       " Node(id: 0a5256, type: NodeType.DOCUMENT, properties: ['page_content', 'document_metadata', 'headlines', 'summary', 'summary_embedding']),\n",
       " Node(id: f6a9e5, type: NodeType.DOCUMENT, properties: ['page_content', 'document_metadata', 'headlines', 'summary', 'summary_embedding']),\n",
       " Node(id: 55746f, type: NodeType.DOCUMENT, properties: ['page_content', 'document_metadata', 'headlines', 'summary', 'summary_embedding']),\n",
       " Node(id: c60096, type: NodeType.DOCUMENT, properties: ['page_content', 'document_metadata', 'headlines', 'summary', 'summary_embedding']),\n",
       " Node(id: 8cb85b, type: NodeType.CHUNK, properties: ['page_content', 'themes', 'entities']),\n",
       " Node(id: a4b129, type: NodeType.CHUNK, properties: ['page_content', 'themes', 'entities']),\n",
       " Node(id: 14101c, type: NodeType.DOCUMENT, properties: ['page_content', 'document_metadata', 'headlines', 'summary', 'summary_embedding']),\n",
       " Node(id: 8eba31, type: NodeType.CHUNK, properties: ['page_content', 'themes', 'entities']),\n",
       " Node(id: 776df5, type: NodeType.DOCUMENT, properties: ['page_content', 'document_metadata', 'headlines', 'summary', 'summary_embedding']),\n",
       " Node(id: e4b2f0, type: NodeType.CHUNK, properties: ['page_content', 'themes', 'entities']),\n",
       " Node(id: 9d4361, type: NodeType.CHUNK, properties: ['page_content', 'themes', 'entities']),\n",
       " Node(id: 08c7fb, type: NodeType.DOCUMENT, properties: ['page_content', 'document_metadata']),\n",
       " Node(id: cea3aa, type: NodeType.DOCUMENT, properties: ['page_content', 'document_metadata']),\n",
       " Node(id: 406f1e, type: NodeType.DOCUMENT, properties: ['page_content', 'document_metadata', 'headlines', 'summary', 'summary_embedding']),\n",
       " Node(id: 666985, type: NodeType.DOCUMENT, properties: ['page_content', 'document_metadata', 'headlines', 'summary', 'summary_embedding']),\n",
       " Node(id: 421d36, type: NodeType.CHUNK, properties: ['page_content', 'themes', 'entities']),\n",
       " Node(id: 61fcbd, type: NodeType.DOCUMENT, properties: ['page_content', 'document_metadata', 'headlines', 'summary', 'summary_embedding']),\n",
       " Node(id: fe875f, type: NodeType.DOCUMENT, properties: ['page_content', 'document_metadata', 'headlines', 'summary', 'summary_embedding'])]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_kg.nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_kg.save(\"test_kg/kg_RelationBuilder_NoDupli.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_kg=KnowledgeGraph().load(\"test_kg/kg_RelationBuilder_NoDupli.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset import TestsetGenerator\n",
    "\n",
    "generator = TestsetGenerator(llm=generator_llm, embedding_model=embedding_model, knowledge_graph=new_kg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.synthesizers import default_query_distribution\n",
    "query_distribution = default_query_distribution(generator_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(SingleHopSpecificQuerySynthesizer(name='single_hop_specifc_query_synthesizer', llm=LangchainLLMWrapper(langchain_llm=ChatOpenAI(...)), generate_query_reference_prompt=QueryAnswerGenerationPrompt(instruction=Generate a single-hop query and answer based on the specified conditions (persona, term, style, length) and the provided context. Ensure the answer is entirely faithful to the context, using only the information directly from the provided context.### Instructions:\n",
       "  1. **Generate a Query**: Based on the context, persona, term, style, and length, create a question that aligns with the persona's perspective and incorporates the term.\n",
       "  2. **Generate an Answer**: Using only the content from the provided context, construct a detailed answer to the query. Do not add any information not included in or inferable from the context.\n",
       "  , examples=[(QueryCondition(persona=Persona(name='Software Engineer', role_description='Focuses on coding best practices and system design.'), term='microservices', query_style='Formal', query_length='Medium', context='Microservices are an architectural style where applications are structured as a collection of loosely coupled services. Each service is fine-grained and focuses on a single functionality.'), GeneratedQueryAnswer(query='What is the purpose of microservices in software architecture?', answer='Microservices are designed to structure applications as a collection of loosely coupled services, each focusing on a single functionality.'))], language=english), theme_persona_matching_prompt=ThemesPersonasMatchingPrompt(instruction=Given a list of themes and personas with their roles, associate each persona with relevant themes based on their role description., examples=[(ThemesPersonasInput(themes=['Empathy', 'Inclusivity', 'Remote work'], personas=[Persona(name='HR Manager', role_description='Focuses on inclusivity and employee support.'), Persona(name='Remote Team Lead', role_description='Manages remote team communication.')]), PersonaThemesMapping(mapping={'HR Manager': ['Inclusivity', 'Empathy'], 'Remote Team Lead': ['Remote work', 'Empathy']}))], language=english), property_name='entities'),\n",
       "  0.3333333333333333),\n",
       " (MultiHopAbstractQuerySynthesizer(name='multi_hop_abstract_query_synthesizer', llm=LangchainLLMWrapper(langchain_llm=ChatOpenAI(...)), generate_query_reference_prompt=QueryAnswerGenerationPrompt(instruction=Generate a multi-hop query and answer based on the specified conditions (persona, themes, style, length) and the provided context. The themes represent a set of phrases either extracted or generated from the context, which highlight the suitability of the selected context for multi-hop query creation. Ensure the query explicitly incorporates these themes.### Instructions:\n",
       "  1. **Generate a Multi-Hop Query**: Use the provided context segments and themes to form a query that requires combining information from multiple segments (e.g., `<1-hop>` and `<2-hop>`). Ensure the query explicitly incorporates one or more themes and reflects their relevance to the context.\n",
       "  2. **Generate an Answer**: Use only the content from the provided context to create a detailed and faithful answer to the query. Avoid adding information that is not directly present or inferable from the given context.\n",
       "  3. **Multi-Hop Context Tags**:\n",
       "     - Each context segment is tagged as `<1-hop>`, `<2-hop>`, etc.\n",
       "     - Ensure the query uses information from at least two segments and connects them meaningfully., examples=[(QueryConditions(persona=Persona(name='Historian', role_description='Focuses on major scientific milestones and their global impact.'), themes=['Theory of Relativity', 'Experimental Validation'], query_style='Formal', query_length='Medium', context=['<1-hop> Albert Einstein developed the theory of relativity, introducing the concept of spacetime.', '<2-hop> The bending of light by gravity was confirmed during the 1919 solar eclipse, supporting Einstein’s theory.']), GeneratedQueryAnswer(query='How was the experimental validation of the theory of relativity achieved during the 1919 solar eclipse?', answer='The experimental validation of the theory of relativity was achieved during the 1919 solar eclipse by confirming the bending of light by gravity, which supported Einstein’s concept of spacetime as proposed in the theory.'))], language=english), concept_combination_prompt=ConceptCombinationPrompt(instruction=Form combinations by pairing concepts from at least two different lists.\n",
       "  **Instructions:**\n",
       "  - Review the concepts from each node.\n",
       "  - Identify concepts that can logically be connected or contrasted.\n",
       "  - Form combinations that involve concepts from different nodes.\n",
       "  - Each combination should include at least one concept from two or more nodes.\n",
       "  - List the combinations clearly and concisely.\n",
       "  - Do not repeat the same combination more than once., examples=[(ConceptsList(lists_of_concepts=[['Artificial intelligence', 'Automation'], ['Healthcare', 'Data privacy']], max_combinations=2), ConceptCombinations(combinations=[['Artificial intelligence', 'Healthcare'], ['Automation', 'Data privacy']]))], language=english), theme_persona_matching_prompt=ThemesPersonasMatchingPrompt(instruction=Given a list of themes and personas with their roles, associate each persona with relevant themes based on their role description., examples=[(ThemesPersonasInput(themes=['Empathy', 'Inclusivity', 'Remote work'], personas=[Persona(name='HR Manager', role_description='Focuses on inclusivity and employee support.'), Persona(name='Remote Team Lead', role_description='Manages remote team communication.')]), PersonaThemesMapping(mapping={'HR Manager': ['Inclusivity', 'Empathy'], 'Remote Team Lead': ['Remote work', 'Empathy']}))], language=english)),\n",
       "  0.3333333333333333),\n",
       " (MultiHopSpecificQuerySynthesizer(name='multi_hop_specific_query_synthesizer', llm=LangchainLLMWrapper(langchain_llm=ChatOpenAI(...)), generate_query_reference_prompt=QueryAnswerGenerationPrompt(instruction=Generate a multi-hop query and answer based on the specified conditions (persona, themes, style, length) and the provided context. The themes represent a set of phrases either extracted or generated from the context, which highlight the suitability of the selected context for multi-hop query creation. Ensure the query explicitly incorporates these themes.### Instructions:\n",
       "  1. **Generate a Multi-Hop Query**: Use the provided context segments and themes to form a query that requires combining information from multiple segments (e.g., `<1-hop>` and `<2-hop>`). Ensure the query explicitly incorporates one or more themes and reflects their relevance to the context.\n",
       "  2. **Generate an Answer**: Use only the content from the provided context to create a detailed and faithful answer to the query. Avoid adding information that is not directly present or inferable from the given context.\n",
       "  3. **Multi-Hop Context Tags**:\n",
       "     - Each context segment is tagged as `<1-hop>`, `<2-hop>`, etc.\n",
       "     - Ensure the query uses information from at least two segments and connects them meaningfully., examples=[(QueryConditions(persona=Persona(name='Historian', role_description='Focuses on major scientific milestones and their global impact.'), themes=['Theory of Relativity', 'Experimental Validation'], query_style='Formal', query_length='Medium', context=['<1-hop> Albert Einstein developed the theory of relativity, introducing the concept of spacetime.', '<2-hop> The bending of light by gravity was confirmed during the 1919 solar eclipse, supporting Einstein’s theory.']), GeneratedQueryAnswer(query='How was the experimental validation of the theory of relativity achieved during the 1919 solar eclipse?', answer='The experimental validation of the theory of relativity was achieved during the 1919 solar eclipse by confirming the bending of light by gravity, which supported Einstein’s concept of spacetime as proposed in the theory.'))], language=english), relation_type='entities_overlap', property_name='entities', theme_persona_matching_prompt=ThemesPersonasMatchingPrompt(instruction=Given a list of themes and personas with their roles, associate each persona with relevant themes based on their role description., examples=[(ThemesPersonasInput(themes=['Empathy', 'Inclusivity', 'Remote work'], personas=[Persona(name='HR Manager', role_description='Focuses on inclusivity and employee support.'), Persona(name='Remote Team Lead', role_description='Manages remote team communication.')]), PersonaThemesMapping(mapping={'HR Manager': ['Inclusivity', 'Empathy'], 'Remote Team Lead': ['Remote work', 'Empathy']}))], language=english)),\n",
       "  0.3333333333333333)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "for query, _ in query_distribution:\n",
    "    prompts = await query.adapt_prompts(\"korean\", llm=generator_llm)\n",
    "    query.set_prompts(**prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(SingleHopSpecificQuerySynthesizer(name='single_hop_specifc_query_synthesizer', llm=LangchainLLMWrapper(langchain_llm=ChatOpenAI(...)), generate_query_reference_prompt=QueryAnswerGenerationPrompt(instruction=Generate a single-hop query and answer based on the specified conditions (persona, term, style, length) and the provided context. Ensure the answer is entirely faithful to the context, using only the information directly from the provided context.### Instructions:\n",
       "  1. **Generate a Query**: Based on the context, persona, term, style, and length, create a question that aligns with the persona's perspective and incorporates the term.\n",
       "  2. **Generate an Answer**: Using only the content from the provided context, construct a detailed answer to the query. Do not add any information not included in or inferable from the context.\n",
       "  , examples=[(QueryCondition(persona=Persona(name='소프트웨어 엔지니어', role_description='코딩 모범 사례와 시스템 설계에 집중합니다.'), term='마이크로서비스', query_style='정식', query_length='중간', context='마이크로서비스는 애플리케이션이 느슨하게 결합된 서비스 모음으로 구성되는 아키텍처 스타일입니다. 각 서비스는 세분화되어 있으며 단일 기능에 집중합니다.'), GeneratedQueryAnswer(query='소프트웨어 아키텍처에서 마이크로서비스의 목적은 무엇입니까?', answer='마이크로서비스는 애플리케이션을 느슨하게 결합된 서비스 모음으로 구조화하도록 설계되었으며, 각 서비스는 단일 기능에 집중합니다.'))], language=korean), theme_persona_matching_prompt=ThemesPersonasMatchingPrompt(instruction=Given a list of themes and personas with their roles, associate each persona with relevant themes based on their role description., examples=[(ThemesPersonasInput(themes=['공감', '포용성', '원격 근무'], personas=[Persona(name='인사 관리자', role_description='포용성과 직원 지원에 중점을 둡니다.'), Persona(name='원격 팀 리드', role_description='원격 팀 커뮤니케이션을 관리합니다.')]), PersonaThemesMapping(mapping={'HR Manager': ['포용성', '공감'], 'Remote Team Lead': ['원격 근무', '공감']}))], language=korean), property_name='entities'),\n",
       "  0.3333333333333333),\n",
       " (MultiHopAbstractQuerySynthesizer(name='multi_hop_abstract_query_synthesizer', llm=LangchainLLMWrapper(langchain_llm=ChatOpenAI(...)), generate_query_reference_prompt=QueryAnswerGenerationPrompt(instruction=Generate a multi-hop query and answer based on the specified conditions (persona, themes, style, length) and the provided context. The themes represent a set of phrases either extracted or generated from the context, which highlight the suitability of the selected context for multi-hop query creation. Ensure the query explicitly incorporates these themes.### Instructions:\n",
       "  1. **Generate a Multi-Hop Query**: Use the provided context segments and themes to form a query that requires combining information from multiple segments (e.g., `<1-hop>` and `<2-hop>`). Ensure the query explicitly incorporates one or more themes and reflects their relevance to the context.\n",
       "  2. **Generate an Answer**: Use only the content from the provided context to create a detailed and faithful answer to the query. Avoid adding information that is not directly present or inferable from the given context.\n",
       "  3. **Multi-Hop Context Tags**:\n",
       "     - Each context segment is tagged as `<1-hop>`, `<2-hop>`, etc.\n",
       "     - Ensure the query uses information from at least two segments and connects them meaningfully., examples=[(QueryConditions(persona=Persona(name='역사학자', role_description='주요 과학적 이정표와 그들의 글로벌 영향에 초점을 맞춥니다.'), themes=['상대성 이론', '실험적 검증'], query_style='형식적', query_length='중간', context=['<1-hop> 알버트 아인슈타인은 상대성 이론을 개발하여 시공간 개념을 도입했습니다.', '<2-hop> 중력에 의한 빛의 굴절은 1919년 일식 동안 확인되어 아인슈타인의 이론을 지지했습니다.']), GeneratedQueryAnswer(query='1919년 일식 동안 상대성 이론의 실험적 검증은 어떻게 이루어졌나요?', answer='상대성 이론의 실험적 검증은 1919년 일식 동안 중력에 의한 빛의 굴절을 확인함으로써 이루어졌으며, 이는 아인슈타인이 제안한 상대성 이론의 시공간 개념을 지지했습니다.'))], language=korean), concept_combination_prompt=ConceptCombinationPrompt(instruction=Form combinations by pairing concepts from at least two different lists.\n",
       "  **Instructions:**\n",
       "  - Review the concepts from each node.\n",
       "  - Identify concepts that can logically be connected or contrasted.\n",
       "  - Form combinations that involve concepts from different nodes.\n",
       "  - Each combination should include at least one concept from two or more nodes.\n",
       "  - List the combinations clearly and concisely.\n",
       "  - Do not repeat the same combination more than once., examples=[(ConceptsList(lists_of_concepts=[['인공지능', '자동화'], ['의료', '데이터 프라이버시']], max_combinations=2), ConceptCombinations(combinations=[['인공지능', '의료'], ['자동화', '데이터 프라이버시']]))], language=korean), theme_persona_matching_prompt=ThemesPersonasMatchingPrompt(instruction=Given a list of themes and personas with their roles, associate each persona with relevant themes based on their role description., examples=[(ThemesPersonasInput(themes=['공감', '포용성', '원격 근무'], personas=[Persona(name='인사 관리자', role_description='포용성과 직원 지원에 중점을 둡니다.'), Persona(name='원격 팀 리드', role_description='원격 팀 커뮤니케이션을 관리합니다.')]), PersonaThemesMapping(mapping={'HR Manager': ['포용성', '공감'], 'Remote Team Lead': ['원격 근무', '공감']}))], language=korean)),\n",
       "  0.3333333333333333),\n",
       " (MultiHopSpecificQuerySynthesizer(name='multi_hop_specific_query_synthesizer', llm=LangchainLLMWrapper(langchain_llm=ChatOpenAI(...)), generate_query_reference_prompt=QueryAnswerGenerationPrompt(instruction=Generate a multi-hop query and answer based on the specified conditions (persona, themes, style, length) and the provided context. The themes represent a set of phrases either extracted or generated from the context, which highlight the suitability of the selected context for multi-hop query creation. Ensure the query explicitly incorporates these themes.### Instructions:\n",
       "  1. **Generate a Multi-Hop Query**: Use the provided context segments and themes to form a query that requires combining information from multiple segments (e.g., `<1-hop>` and `<2-hop>`). Ensure the query explicitly incorporates one or more themes and reflects their relevance to the context.\n",
       "  2. **Generate an Answer**: Use only the content from the provided context to create a detailed and faithful answer to the query. Avoid adding information that is not directly present or inferable from the given context.\n",
       "  3. **Multi-Hop Context Tags**:\n",
       "     - Each context segment is tagged as `<1-hop>`, `<2-hop>`, etc.\n",
       "     - Ensure the query uses information from at least two segments and connects them meaningfully., examples=[(QueryConditions(persona=Persona(name='역사학자', role_description='주요 과학적 이정표와 그들의 세계적 영향을 집중적으로 다룹니다.'), themes=['상대성 이론', '실험적 검증'], query_style='형식적', query_length='중간', context=['<1-hop> 알버트 아인슈타인은 상대성 이론을 개발하여 시공간 개념을 도입했습니다.', '<2-hop> 중력에 의한 빛의 굴절은 1919년 일식 동안 확인되어 아인슈타인의 이론을 지지했습니다.']), GeneratedQueryAnswer(query='1919년 일식 동안 상대성 이론의 실험적 검증은 어떻게 이루어졌나요?', answer='상대성 이론의 실험적 검증은 1919년 일식 동안 중력에 의한 빛의 굴절을 확인함으로써 이루어졌으며, 이는 아인슈타인이 제안한 시공간 개념을 지지했습니다.'))], language=korean), relation_type='entities_overlap', property_name='entities', theme_persona_matching_prompt=ThemesPersonasMatchingPrompt(instruction=Given a list of themes and personas with their roles, associate each persona with relevant themes based on their role description., examples=[(ThemesPersonasInput(themes=['공감', '포용성', '원격 근무'], personas=[Persona(name='인사 관리자', role_description='포용성과 직원 지원에 중점을 둡니다.'), Persona(name='원격 팀 리드', role_description='원격 팀 커뮤니케이션을 관리합니다.')]), PersonaThemesMapping(mapping={'HR Manager': ['포용성', '공감'], 'Remote Team Lead': ['원격 근무', '공감']}))], language=korean)),\n",
       "  0.3333333333333333)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.persona import default_filter, generate_personas_from_kg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([node for node in new_kg.nodes if default_filter(node)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating personas:   0%|          | 0/23 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating personas: 100%|██████████| 23/23 [00:03<00:00,  7.53it/s]\n"
     ]
    }
   ],
   "source": [
    "persona_list=generate_personas_from_kg(\n",
    "    llm = generator_llm,\n",
    "    kg = new_kg,\n",
    "    num_personas=23\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Persona(name='Financial Data Scientist', role_description='Focuses on utilizing domain-specific language models to analyze and interpret financial datasets, ensuring compliance and security in the application of natural language processing within the financial sector.'),\n",
       " Persona(name='Machine Learning Engineer', role_description='Specializes in setting up and optimizing the training environment for deep learning models, considering key factors like batch size, hardware selection, and hyperparameters.'),\n",
       " Persona(name='Financial Data Analyst', role_description='Specializes in the collection and analysis of financial datasets to derive insights for market trends and product understanding.'),\n",
       " Persona(name='AI Research Scientist', role_description='Specializes in improving machine learning models through techniques like fine-tuning and matrix decomposition, focusing on advancing NLP and NLG capabilities.'),\n",
       " Persona(name='Financial Data Scientist', role_description='Utilizes LLMs to enhance decision-making and operational efficiency in the finance sector through model fine-tuning and performance evaluation.'),\n",
       " Persona(name='Financial Data Analyst', role_description='Specializes in evaluating and interpreting financial models to ensure accurate predictions and reporting.'),\n",
       " Persona(name='AI Product Manager', role_description='Oversees the development and implementation of AI solutions, focusing on customer satisfaction and practical applications of AI technologies.'),\n",
       " Persona(name='Financial Data Analyst', role_description='Specializes in enhancing model performance for financial predictions and client interactions through advanced data techniques.'),\n",
       " Persona(name='Financial Technology Analyst', role_description='Utilizes AI and LLM technologies to enhance document processing, contract analysis, and financial research in the financial sector.'),\n",
       " Persona(name='Financial Data Scientist', role_description='Specializes in natural language processing for sentiment analysis in financial news, focusing on model optimization and hyperparameter tuning.'),\n",
       " Persona(name='AI Research Engineer', role_description='Specializes in the development and fine-tuning of language models for specific domains, focusing on efficiency and performance enhancements.'),\n",
       " Persona(name='Financial Data Analyst', role_description='Specializes in analyzing financial sentiment and enhancing product recommendations through tailored data processing techniques.'),\n",
       " Persona(name='Financial Technology Analyst', role_description='Focuses on the integration of AI-driven language models in the finance sector, particularly in optimizing customer interactions and addressing data privacy concerns.'),\n",
       " Persona(name='AI Research Engineer', role_description='Specializes in fine-tuning language models for specific industries, such as insurance, to enhance their performance and applicability.'),\n",
       " Persona(name='Insurance Technology Analyst', role_description='Evaluates AI advancements for enhancing efficiency and customer experience in the insurance sector.'),\n",
       " Persona(name='AI Product Manager', role_description='Oversees the development and integration of AI language models, staying informed about market competition and technological advancements.'),\n",
       " Persona(name='Financial Data Scientist', role_description='Applies machine learning and data preprocessing techniques to develop specialized language models for the financial sector.'),\n",
       " Persona(name='Financial Data Scientist', role_description='Works on fine-tuning language models to interpret financial data, focusing on terminology, numerical processing, and understanding complex rules.'),\n",
       " Persona(name='Financial Data Scientist', role_description='Specializes in analyzing financial data to enhance prediction accuracy while ensuring compliance with security and privacy regulations.'),\n",
       " Persona(name='AI Research Analyst', role_description='Analyzes advancements in large language models and generative AI, focusing on their applications and ethical implications in various fields.'),\n",
       " Persona(name='Financial Data Analyst', role_description='Specializes in utilizing advanced language models to analyze financial data and conduct predictive analytics.'),\n",
       " Persona(name='AI Researcher', role_description='Engages in the development and optimization of language models to innovate solutions in AI applications.'),\n",
       " Persona(name='Insurance Technology Analyst', role_description='Evaluates AI applications in the insurance industry to enhance customer experience and business productivity.')]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "persona_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Scenarios: 100%|██████████| 3/3 [00:23<00:00,  7.71s/it]\n",
      "Generating Samples: 100%|██████████| 22/22 [00:09<00:00,  2.24it/s]\n"
     ]
    }
   ],
   "source": [
    "testset_generator = TestsetGenerator(embedding_model=ragas_embeddings, llm=generator_llm, knowledge_graph=new_kg, persona_list=persona_list)\n",
    "samples = testset_generator.generate(testset_size=20, query_distribution=query_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference</th>\n",
       "      <th>synthesizer_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What evaluation benchmarks are used for Bloomb...</td>\n",
       "      <td>[&lt;표 7&gt; Evaluation Benchmarks of BloombergGPT. ...</td>\n",
       "      <td>BloombergGPT uses several evaluation benchmark...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>금융 특화 파인튜닝이란 무엇이며, 어떤 성능을 보이나요?</td>\n",
       "      <td>[하여, 금융 분야의 특정 작업에 높은 성능을 3.4.2. 금융 특화 파인튜닝 LL...</td>\n",
       "      <td>금융 특화 파인튜닝은 금융 분야의 특정 작업에 높은 성능을 보이는 LLM(대형 언어...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>금융 리서치에서 LLM을 활용하는 방법과 그 필요성에 대해 설명해줄 수 있나요?</td>\n",
       "      <td>[금융 데이터의 특성을 고려한 파인튜닝을 통해 된 데이터셋은 생성방법을 가이드하기위...</td>\n",
       "      <td>금융 리서치에서 LLM을 활용하는 방법은 다양한 금융 데이터셋을 확보하여 학습하고,...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FinGPT는 금융 분야에서 어떤 역할을 하며, 보안 및 규정 준수와 관련하여 어떤...</td>\n",
       "      <td>[3.4. 금융 분야를 위한 LLM 구성 3.3.3. 보안 및 규정 준수 금융 분야...</td>\n",
       "      <td>FinGPT는 금융 분야에서 자연어 처리에 특화된 모델로, 금융 데이터의 민감한 정...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>미래에셋증권은 어떤 서비스를 제공하고 있습니까?</td>\n",
       "      <td>[수 있다. 예를 들어, 모델은 금융 뉴스, 기업 보고 4.2.4. 자동화된 금융 ...</td>\n",
       "      <td>미래에셋증권은 챗 GPT를 활용해 종목의 시황을 요약하는 서비스 작성을 도입하여 금...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>정천수는 무엇을 의미하나요?</td>\n",
       "      <td>[사전학습하기 위한 사례에 맞게 파인튜닝을 하게 된다(정천수, 2023d). &lt;그림...</td>\n",
       "      <td>정천수는 사전학습하기 위한 사례에 맞게 파인튜닝을 하게 되는 과정을 나타냅니다.</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>자동 응답 시스템이 금융 분야에서 고객 응대 품질을 향상시키는 데 어떤 역할을 할 ...</td>\n",
       "      <td>[고객 응대 개선측 종합적으로, 본 연구는 LLM을 금융 분야에 면에서는 자동 응답...</td>\n",
       "      <td>자동 응답 시스템은 LLM을 금융 분야에 적용하여 고객 응대의 품질을 향상시키는 데...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>금융 분야에서 LLM의 파인튜닝은 어떻게 이루어지며, FinGPT와 Bloomber...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\n하여, 금융 분야의 특정 작업에 높은 성능을 3.4.2. 금융...</td>\n",
       "      <td>금융 분야에서 LLM의 파인튜닝은 특정 작업에 높은 성능을 보이기 위해 금융 특화된...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>금융 분야에서 LLM 파인튜닝이 어떻게 이루어지고 있으며, 어떤 모델들이 사용되고 ...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\n하여, 금융 분야의 특정 작업에 높은 성능을 3.4.2. 금융...</td>\n",
       "      <td>금융 분야에서 LLM 파인튜닝은 특정 작업에 높은 성능을 보이기 위해 금융 특화된 ...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>금융 분야에서 모델 성능 극대화를 위해 하이퍼파라미터 조정은 어떻게 이루어지나요?</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\n2) 모델의 용도: 모델의 용도에 따라 적합한 3.2.2. 하...</td>\n",
       "      <td>금융 분야에서 모델 성능 극대화를 위해 하이퍼파라미터 조정은 모델의 용도에 따라 적...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>금융 분야에서 LLM의 파인튜닝은 어떤 모델들이 있으며, 그들의 성능은 어떻게 평가...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\n하여, 금융 분야의 특정 작업에 높은 성능을 3.4.2. 금융...</td>\n",
       "      <td>금융 분야에서 LLM의 파인튜닝은 여러 모델들이 있으며, 그 중 FinGPT와 Fi...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>금융 분야에서 모델의 용도에 따라 하이퍼파라미터 조정은 어떻게 이루어지나요?</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\n2) 모델의 용도: 모델의 용도에 따라 적합한 3.2.2. 하...</td>\n",
       "      <td>금융 분야에서 모델의 용도에 따라 하이퍼파라미터 조정은 모델의 성능을 극대화하기 위...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>금융 분야에서 LLM 파인튜닝의 중요성과 FinGPT, BloombergGPT, F...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\n하여, 금융 분야의 특정 작업에 높은 성능을 3.4.2. 금융...</td>\n",
       "      <td>금융 분야에서 LLM 파인튜닝은 특정 작업에 높은 성능을 보이는 모델을 개발하는 데...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>금융 분야에서 모델의 용도에 따라 하이퍼파라미터 조정이 어떻게 이루어지며, 금융 특...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\n2) 모델의 용도: 모델의 용도에 따라 적합한 3.2.2. 하...</td>\n",
       "      <td>모델의 용도에 따라 적합한 하이퍼파라미터 조정이 이루어지며, 예를 들어 금융 뉴스 ...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>FinGPT는 금융 분야에서 어떤 특화된 기능을 가지고 있나요?</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\n3.4. 금융 분야를 위한 LLM 구성 3.3.3. 보안 및 ...</td>\n",
       "      <td>FinGPT는 금융 도메인에 특화된 데이터셋과 파인튜닝을 통해 높은 성능을 달성하며...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>FinGPT는 금융 분야에서 어떤 방식으로 파인튜닝되어 있으며, BloombergG...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\n3.4. 금융 분야를 위한 LLM 구성 3.3.3. 보안 및 ...</td>\n",
       "      <td>FinGPT는 OpenAI의 GPT 아키텍처를 기반으로 하여 금융 도메인에서 처음부...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>GPT-4와 FinGPT의 금융 도메인에서의 차이점은 무엇인가요?</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\n모델의 가용성: 모델이 공개되어 있지 않다 적절히 조정함으로써...</td>\n",
       "      <td>GPT-4는 OpenAI에서 개발한 대규모 언어 모델로, 1,750억 개 이상의 매...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>FinBERT와 BloombergGPT의 금융 분야에서의 성능 차이는 무엇인가요?</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\n모델의 가용성: 모델이 공개되어 있지 않다 적절히 조정함으로써...</td>\n",
       "      <td>FinBERT는 BERT 아키텍처를 사용하여 금융 데이터로 사전 학습되었으며, 주로...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>BloombergGPT와 BLOOM 모델의 금융 특화 파인튜닝은 어떤 차이점이 있나요?</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\n&lt;표 7&gt; Evaluation Benchmarks of Blo...</td>\n",
       "      <td>BloombergGPT는 금융 전문 언어를 학습한 GPT 기반 모델로, 예측 분석 ...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>BloombergGPT와 BLOOM 모델의 금융 분야에서의 성능 차이는 무엇인가요?</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\n&lt;표 7&gt; Evaluation Benchmarks of Blo...</td>\n",
       "      <td>BloombergGPT는 금융 전문 언어를 학습한 GPT 기반 모델로, 예측 분석과...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Mistral-7B SLM의 금융 분야에서의 활용 가능성과 파인튜닝 방법은 무엇인가요?</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\n모델의 가용성: 모델이 공개되어 있지 않다 적절히 조정함으로써...</td>\n",
       "      <td>Mistral-7B SLM은 금융 분야에서의 다양한 작업에 활용될 수 있으며, 파인...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Mistral-7B SLM의 금융 데이터에 대한 파인튜닝은 어떻게 이루어지며, 이 ...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\n모델의 가용성: 모델이 공개되어 있지 않다 적절히 조정함으로써...</td>\n",
       "      <td>Mistral-7B SLM의 금융 데이터에 대한 파인튜닝은 금융 데이터의 특성을 고...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           user_input  \\\n",
       "0   What evaluation benchmarks are used for Bloomb...   \n",
       "1                     금융 특화 파인튜닝이란 무엇이며, 어떤 성능을 보이나요?   \n",
       "2        금융 리서치에서 LLM을 활용하는 방법과 그 필요성에 대해 설명해줄 수 있나요?   \n",
       "3   FinGPT는 금융 분야에서 어떤 역할을 하며, 보안 및 규정 준수와 관련하여 어떤...   \n",
       "4                          미래에셋증권은 어떤 서비스를 제공하고 있습니까?   \n",
       "5                                     정천수는 무엇을 의미하나요?   \n",
       "6   자동 응답 시스템이 금융 분야에서 고객 응대 품질을 향상시키는 데 어떤 역할을 할 ...   \n",
       "7   금융 분야에서 LLM의 파인튜닝은 어떻게 이루어지며, FinGPT와 Bloomber...   \n",
       "8   금융 분야에서 LLM 파인튜닝이 어떻게 이루어지고 있으며, 어떤 모델들이 사용되고 ...   \n",
       "9       금융 분야에서 모델 성능 극대화를 위해 하이퍼파라미터 조정은 어떻게 이루어지나요?   \n",
       "10  금융 분야에서 LLM의 파인튜닝은 어떤 모델들이 있으며, 그들의 성능은 어떻게 평가...   \n",
       "11         금융 분야에서 모델의 용도에 따라 하이퍼파라미터 조정은 어떻게 이루어지나요?   \n",
       "12  금융 분야에서 LLM 파인튜닝의 중요성과 FinGPT, BloombergGPT, F...   \n",
       "13  금융 분야에서 모델의 용도에 따라 하이퍼파라미터 조정이 어떻게 이루어지며, 금융 특...   \n",
       "14                FinGPT는 금융 분야에서 어떤 특화된 기능을 가지고 있나요?   \n",
       "15  FinGPT는 금융 분야에서 어떤 방식으로 파인튜닝되어 있으며, BloombergG...   \n",
       "16               GPT-4와 FinGPT의 금융 도메인에서의 차이점은 무엇인가요?   \n",
       "17      FinBERT와 BloombergGPT의 금융 분야에서의 성능 차이는 무엇인가요?   \n",
       "18   BloombergGPT와 BLOOM 모델의 금융 특화 파인튜닝은 어떤 차이점이 있나요?   \n",
       "19     BloombergGPT와 BLOOM 모델의 금융 분야에서의 성능 차이는 무엇인가요?   \n",
       "20   Mistral-7B SLM의 금융 분야에서의 활용 가능성과 파인튜닝 방법은 무엇인가요?   \n",
       "21  Mistral-7B SLM의 금융 데이터에 대한 파인튜닝은 어떻게 이루어지며, 이 ...   \n",
       "\n",
       "                                   reference_contexts  \\\n",
       "0   [<표 7> Evaluation Benchmarks of BloombergGPT. ...   \n",
       "1   [하여, 금융 분야의 특정 작업에 높은 성능을 3.4.2. 금융 특화 파인튜닝 LL...   \n",
       "2   [금융 데이터의 특성을 고려한 파인튜닝을 통해 된 데이터셋은 생성방법을 가이드하기위...   \n",
       "3   [3.4. 금융 분야를 위한 LLM 구성 3.3.3. 보안 및 규정 준수 금융 분야...   \n",
       "4   [수 있다. 예를 들어, 모델은 금융 뉴스, 기업 보고 4.2.4. 자동화된 금융 ...   \n",
       "5   [사전학습하기 위한 사례에 맞게 파인튜닝을 하게 된다(정천수, 2023d). <그림...   \n",
       "6   [고객 응대 개선측 종합적으로, 본 연구는 LLM을 금융 분야에 면에서는 자동 응답...   \n",
       "7   [<1-hop>\\n\\n하여, 금융 분야의 특정 작업에 높은 성능을 3.4.2. 금융...   \n",
       "8   [<1-hop>\\n\\n하여, 금융 분야의 특정 작업에 높은 성능을 3.4.2. 금융...   \n",
       "9   [<1-hop>\\n\\n2) 모델의 용도: 모델의 용도에 따라 적합한 3.2.2. 하...   \n",
       "10  [<1-hop>\\n\\n하여, 금융 분야의 특정 작업에 높은 성능을 3.4.2. 금융...   \n",
       "11  [<1-hop>\\n\\n2) 모델의 용도: 모델의 용도에 따라 적합한 3.2.2. 하...   \n",
       "12  [<1-hop>\\n\\n하여, 금융 분야의 특정 작업에 높은 성능을 3.4.2. 금융...   \n",
       "13  [<1-hop>\\n\\n2) 모델의 용도: 모델의 용도에 따라 적합한 3.2.2. 하...   \n",
       "14  [<1-hop>\\n\\n3.4. 금융 분야를 위한 LLM 구성 3.3.3. 보안 및 ...   \n",
       "15  [<1-hop>\\n\\n3.4. 금융 분야를 위한 LLM 구성 3.3.3. 보안 및 ...   \n",
       "16  [<1-hop>\\n\\n모델의 가용성: 모델이 공개되어 있지 않다 적절히 조정함으로써...   \n",
       "17  [<1-hop>\\n\\n모델의 가용성: 모델이 공개되어 있지 않다 적절히 조정함으로써...   \n",
       "18  [<1-hop>\\n\\n<표 7> Evaluation Benchmarks of Blo...   \n",
       "19  [<1-hop>\\n\\n<표 7> Evaluation Benchmarks of Blo...   \n",
       "20  [<1-hop>\\n\\n모델의 가용성: 모델이 공개되어 있지 않다 적절히 조정함으로써...   \n",
       "21  [<1-hop>\\n\\n모델의 가용성: 모델이 공개되어 있지 않다 적절히 조정함으로써...   \n",
       "\n",
       "                                            reference  \\\n",
       "0   BloombergGPT uses several evaluation benchmark...   \n",
       "1   금융 특화 파인튜닝은 금융 분야의 특정 작업에 높은 성능을 보이는 LLM(대형 언어...   \n",
       "2   금융 리서치에서 LLM을 활용하는 방법은 다양한 금융 데이터셋을 확보하여 학습하고,...   \n",
       "3   FinGPT는 금융 분야에서 자연어 처리에 특화된 모델로, 금융 데이터의 민감한 정...   \n",
       "4   미래에셋증권은 챗 GPT를 활용해 종목의 시황을 요약하는 서비스 작성을 도입하여 금...   \n",
       "5        정천수는 사전학습하기 위한 사례에 맞게 파인튜닝을 하게 되는 과정을 나타냅니다.   \n",
       "6   자동 응답 시스템은 LLM을 금융 분야에 적용하여 고객 응대의 품질을 향상시키는 데...   \n",
       "7   금융 분야에서 LLM의 파인튜닝은 특정 작업에 높은 성능을 보이기 위해 금융 특화된...   \n",
       "8   금융 분야에서 LLM 파인튜닝은 특정 작업에 높은 성능을 보이기 위해 금융 특화된 ...   \n",
       "9   금융 분야에서 모델 성능 극대화를 위해 하이퍼파라미터 조정은 모델의 용도에 따라 적...   \n",
       "10  금융 분야에서 LLM의 파인튜닝은 여러 모델들이 있으며, 그 중 FinGPT와 Fi...   \n",
       "11  금융 분야에서 모델의 용도에 따라 하이퍼파라미터 조정은 모델의 성능을 극대화하기 위...   \n",
       "12  금융 분야에서 LLM 파인튜닝은 특정 작업에 높은 성능을 보이는 모델을 개발하는 데...   \n",
       "13  모델의 용도에 따라 적합한 하이퍼파라미터 조정이 이루어지며, 예를 들어 금융 뉴스 ...   \n",
       "14  FinGPT는 금융 도메인에 특화된 데이터셋과 파인튜닝을 통해 높은 성능을 달성하며...   \n",
       "15  FinGPT는 OpenAI의 GPT 아키텍처를 기반으로 하여 금융 도메인에서 처음부...   \n",
       "16  GPT-4는 OpenAI에서 개발한 대규모 언어 모델로, 1,750억 개 이상의 매...   \n",
       "17  FinBERT는 BERT 아키텍처를 사용하여 금융 데이터로 사전 학습되었으며, 주로...   \n",
       "18  BloombergGPT는 금융 전문 언어를 학습한 GPT 기반 모델로, 예측 분석 ...   \n",
       "19  BloombergGPT는 금융 전문 언어를 학습한 GPT 기반 모델로, 예측 분석과...   \n",
       "20  Mistral-7B SLM은 금융 분야에서의 다양한 작업에 활용될 수 있으며, 파인...   \n",
       "21  Mistral-7B SLM의 금융 데이터에 대한 파인튜닝은 금융 데이터의 특성을 고...   \n",
       "\n",
       "                        synthesizer_name  \n",
       "0   single_hop_specifc_query_synthesizer  \n",
       "1   single_hop_specifc_query_synthesizer  \n",
       "2   single_hop_specifc_query_synthesizer  \n",
       "3   single_hop_specifc_query_synthesizer  \n",
       "4   single_hop_specifc_query_synthesizer  \n",
       "5   single_hop_specifc_query_synthesizer  \n",
       "6   single_hop_specifc_query_synthesizer  \n",
       "7   multi_hop_abstract_query_synthesizer  \n",
       "8   multi_hop_abstract_query_synthesizer  \n",
       "9   multi_hop_abstract_query_synthesizer  \n",
       "10  multi_hop_abstract_query_synthesizer  \n",
       "11  multi_hop_abstract_query_synthesizer  \n",
       "12  multi_hop_abstract_query_synthesizer  \n",
       "13  multi_hop_abstract_query_synthesizer  \n",
       "14  multi_hop_specific_query_synthesizer  \n",
       "15  multi_hop_specific_query_synthesizer  \n",
       "16  multi_hop_specific_query_synthesizer  \n",
       "17  multi_hop_specific_query_synthesizer  \n",
       "18  multi_hop_specific_query_synthesizer  \n",
       "19  multi_hop_specific_query_synthesizer  \n",
       "20  multi_hop_specific_query_synthesizer  \n",
       "21  multi_hop_specific_query_synthesizer  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df = samples.to_pandas()\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 테스트셋 체크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm_gemini = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "class QA_check(BaseModel):\n",
    "    validity: bool\n",
    "    reference_context_relevance: bool\n",
    "    qa_appropriateness:bool \n",
    "    \n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=QA_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "다음 질문과 답변이 주어진 논문을 기준으로 적절한지 평가하십시오.\n",
    "\n",
    "다음 항목을 순차적으로 평가해 주세요:\n",
    "\n",
    "1. 질문의 적절성 (Validity)\n",
    "- 질문이 논문에 언급된 용어, 표현, 개념에 대해 논문에서 제공하는 정보를 바탕으로 답할 수 있도록 명확히 구성되었나요?\n",
    "- 논문에서 인용된 사람의 이름 또는 특정 저자명을 단순 인용 표기(예: (홍길동, 2024))임에도 불구하고 단어나 개념처럼 잘못 해석하여 그 의미를 묻는 질문은 부적절합니다.\n",
    "(예: 논문 인용 표기에서 \"(홍길동, 2024)\"와 같은 저자 이름을 하나의 개념이나 용어로 잘못 파악하고 질문하는 경우는 잘못된 질문임.)\n",
    "\n",
    "2. 참조 문맥의 관련성 (Reference Context Relevance)\n",
    "- 제공된 참조 문맥이 질문과 답변에 관련 있으며, 논문의 실제 내용과 정확히 일치하는가요?\n",
    "\n",
    "3. 답변의 적합성 (QA Appropriateness)\n",
    "- 제공된 답변이 논문에서 실제로 제시된 정보를 근거로 질문에 적절히 대응하는가요?\n",
    "\n",
    "# 질문\n",
    "{user_input}\n",
    "\n",
    "# 답변\n",
    "{reference}\n",
    "\n",
    "# 참조 문맥\n",
    "{reference_contexts}\n",
    "\n",
    "# 포맷\n",
    "```json\n",
    "{{\"validity\": true or false, \"reference_context_relevance\": true or false, \"qa_appropriateness\": true or false}}\n",
    "```\n",
    "\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RunnablePassthrough()|PromptTemplate(template=prompt, input_variables=[\"user_input\", \"reference\", \"reference_contexts\"])|llm_gemini|parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_input': '정천수는 무엇을 의미하나요?',\n",
       " 'reference_contexts': ['사전학습하기 위한 사례에 맞게 파인튜닝을 하게 된다(정천수, 2023d). <그림 3> LLM 사전 학습 절차 99'],\n",
       " 'reference': '정천수는 사전학습하기 위한 사례에 맞게 파인튜닝을 하게 되는 과정을 나타냅니다.'}"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_sample = sample_df.loc[5,:\"reference\"]\n",
    "check_sample.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_check = chain.invoke(check_sample.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QA_check(validity=False, reference_context_relevance=True, qa_appropriateness=False)"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10\n",
      "10 20\n",
      "20 30\n"
     ]
    }
   ],
   "source": [
    "ans_total = []\n",
    "for sample_start in range(0,sample_df.shape[0],10):\n",
    "    print(sample_start, sample_start+10)\n",
    "    ans = chain.batch(list(sample_df.loc[sample_start:sample_start+10,:\"reference\"].T.to_dict().values()))\n",
    "    ans_total.extend(ans)\n",
    "    time.sleep(60.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(sample_df.loc[sample_start:sample_start+10,:\"reference\"].T.to_dict().values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(ans_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference</th>\n",
       "      <th>synthesizer_name</th>\n",
       "      <th>validity</th>\n",
       "      <th>reference_context_relevance</th>\n",
       "      <th>qa_appropriateness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What evaluation benchmarks are used for Bloomb...</td>\n",
       "      <td>[&lt;표 7&gt; Evaluation Benchmarks of BloombergGPT. ...</td>\n",
       "      <td>BloombergGPT uses several evaluation benchmark...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>금융 특화 파인튜닝이란 무엇이며, 어떤 성능을 보이나요?</td>\n",
       "      <td>[하여, 금융 분야의 특정 작업에 높은 성능을 3.4.2. 금융 특화 파인튜닝 LL...</td>\n",
       "      <td>금융 특화 파인튜닝은 금융 분야의 특정 작업에 높은 성능을 보이는 LLM(대형 언어...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>금융 리서치에서 LLM을 활용하는 방법과 그 필요성에 대해 설명해줄 수 있나요?</td>\n",
       "      <td>[금융 데이터의 특성을 고려한 파인튜닝을 통해 된 데이터셋은 생성방법을 가이드하기위...</td>\n",
       "      <td>금융 리서치에서 LLM을 활용하는 방법은 다양한 금융 데이터셋을 확보하여 학습하고,...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FinGPT는 금융 분야에서 어떤 역할을 하며, 보안 및 규정 준수와 관련하여 어떤...</td>\n",
       "      <td>[3.4. 금융 분야를 위한 LLM 구성 3.3.3. 보안 및 규정 준수 금융 분야...</td>\n",
       "      <td>FinGPT는 금융 분야에서 자연어 처리에 특화된 모델로, 금융 데이터의 민감한 정...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>미래에셋증권은 어떤 서비스를 제공하고 있습니까?</td>\n",
       "      <td>[수 있다. 예를 들어, 모델은 금융 뉴스, 기업 보고 4.2.4. 자동화된 금융 ...</td>\n",
       "      <td>미래에셋증권은 챗 GPT를 활용해 종목의 시황을 요약하는 서비스 작성을 도입하여 금...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>정천수는 무엇을 의미하나요?</td>\n",
       "      <td>[사전학습하기 위한 사례에 맞게 파인튜닝을 하게 된다(정천수, 2023d). &lt;그림...</td>\n",
       "      <td>정천수는 사전학습하기 위한 사례에 맞게 파인튜닝을 하게 되는 과정을 나타냅니다.</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>자동 응답 시스템이 금융 분야에서 고객 응대 품질을 향상시키는 데 어떤 역할을 할 ...</td>\n",
       "      <td>[고객 응대 개선측 종합적으로, 본 연구는 LLM을 금융 분야에 면에서는 자동 응답...</td>\n",
       "      <td>자동 응답 시스템은 LLM을 금융 분야에 적용하여 고객 응대의 품질을 향상시키는 데...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>금융 분야에서 LLM의 파인튜닝은 어떻게 이루어지며, FinGPT와 Bloomber...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\n하여, 금융 분야의 특정 작업에 높은 성능을 3.4.2. 금융...</td>\n",
       "      <td>금융 분야에서 LLM의 파인튜닝은 특정 작업에 높은 성능을 보이기 위해 금융 특화된...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>금융 분야에서 LLM 파인튜닝이 어떻게 이루어지고 있으며, 어떤 모델들이 사용되고 ...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\n하여, 금융 분야의 특정 작업에 높은 성능을 3.4.2. 금융...</td>\n",
       "      <td>금융 분야에서 LLM 파인튜닝은 특정 작업에 높은 성능을 보이기 위해 금융 특화된 ...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>금융 분야에서 모델 성능 극대화를 위해 하이퍼파라미터 조정은 어떻게 이루어지나요?</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\n2) 모델의 용도: 모델의 용도에 따라 적합한 3.2.2. 하...</td>\n",
       "      <td>금융 분야에서 모델 성능 극대화를 위해 하이퍼파라미터 조정은 모델의 용도에 따라 적...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>금융 분야에서 LLM의 파인튜닝은 어떤 모델들이 있으며, 그들의 성능은 어떻게 평가...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\n하여, 금융 분야의 특정 작업에 높은 성능을 3.4.2. 금융...</td>\n",
       "      <td>금융 분야에서 LLM의 파인튜닝은 여러 모델들이 있으며, 그 중 FinGPT와 Fi...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>금융 분야에서 모델의 용도에 따라 하이퍼파라미터 조정은 어떻게 이루어지나요?</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\n2) 모델의 용도: 모델의 용도에 따라 적합한 3.2.2. 하...</td>\n",
       "      <td>금융 분야에서 모델의 용도에 따라 하이퍼파라미터 조정은 모델의 성능을 극대화하기 위...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>금융 분야에서 LLM 파인튜닝의 중요성과 FinGPT, BloombergGPT, F...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\n하여, 금융 분야의 특정 작업에 높은 성능을 3.4.2. 금융...</td>\n",
       "      <td>금융 분야에서 LLM 파인튜닝은 특정 작업에 높은 성능을 보이는 모델을 개발하는 데...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>금융 분야에서 모델의 용도에 따라 하이퍼파라미터 조정이 어떻게 이루어지며, 금융 특...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\n2) 모델의 용도: 모델의 용도에 따라 적합한 3.2.2. 하...</td>\n",
       "      <td>모델의 용도에 따라 적합한 하이퍼파라미터 조정이 이루어지며, 예를 들어 금융 뉴스 ...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>FinGPT는 금융 분야에서 어떤 특화된 기능을 가지고 있나요?</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\n3.4. 금융 분야를 위한 LLM 구성 3.3.3. 보안 및 ...</td>\n",
       "      <td>FinGPT는 금융 도메인에 특화된 데이터셋과 파인튜닝을 통해 높은 성능을 달성하며...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>FinGPT는 금융 분야에서 어떤 방식으로 파인튜닝되어 있으며, BloombergG...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\n3.4. 금융 분야를 위한 LLM 구성 3.3.3. 보안 및 ...</td>\n",
       "      <td>FinGPT는 OpenAI의 GPT 아키텍처를 기반으로 하여 금융 도메인에서 처음부...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>GPT-4와 FinGPT의 금융 도메인에서의 차이점은 무엇인가요?</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\n모델의 가용성: 모델이 공개되어 있지 않다 적절히 조정함으로써...</td>\n",
       "      <td>GPT-4는 OpenAI에서 개발한 대규모 언어 모델로, 1,750억 개 이상의 매...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>FinBERT와 BloombergGPT의 금융 분야에서의 성능 차이는 무엇인가요?</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\n모델의 가용성: 모델이 공개되어 있지 않다 적절히 조정함으로써...</td>\n",
       "      <td>FinBERT는 BERT 아키텍처를 사용하여 금융 데이터로 사전 학습되었으며, 주로...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>BloombergGPT와 BLOOM 모델의 금융 특화 파인튜닝은 어떤 차이점이 있나요?</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\n&lt;표 7&gt; Evaluation Benchmarks of Blo...</td>\n",
       "      <td>BloombergGPT는 금융 전문 언어를 학습한 GPT 기반 모델로, 예측 분석 ...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>BloombergGPT와 BLOOM 모델의 금융 분야에서의 성능 차이는 무엇인가요?</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\n&lt;표 7&gt; Evaluation Benchmarks of Blo...</td>\n",
       "      <td>BloombergGPT는 금융 전문 언어를 학습한 GPT 기반 모델로, 예측 분석과...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Mistral-7B SLM의 금융 분야에서의 활용 가능성과 파인튜닝 방법은 무엇인가요?</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\n모델의 가용성: 모델이 공개되어 있지 않다 적절히 조정함으로써...</td>\n",
       "      <td>Mistral-7B SLM은 금융 분야에서의 다양한 작업에 활용될 수 있으며, 파인...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Mistral-7B SLM의 금융 데이터에 대한 파인튜닝은 어떻게 이루어지며, 이 ...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\n모델의 가용성: 모델이 공개되어 있지 않다 적절히 조정함으로써...</td>\n",
       "      <td>Mistral-7B SLM의 금융 데이터에 대한 파인튜닝은 금융 데이터의 특성을 고...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           user_input  \\\n",
       "0   What evaluation benchmarks are used for Bloomb...   \n",
       "1                     금융 특화 파인튜닝이란 무엇이며, 어떤 성능을 보이나요?   \n",
       "2        금융 리서치에서 LLM을 활용하는 방법과 그 필요성에 대해 설명해줄 수 있나요?   \n",
       "3   FinGPT는 금융 분야에서 어떤 역할을 하며, 보안 및 규정 준수와 관련하여 어떤...   \n",
       "4                          미래에셋증권은 어떤 서비스를 제공하고 있습니까?   \n",
       "5                                     정천수는 무엇을 의미하나요?   \n",
       "6   자동 응답 시스템이 금융 분야에서 고객 응대 품질을 향상시키는 데 어떤 역할을 할 ...   \n",
       "7   금융 분야에서 LLM의 파인튜닝은 어떻게 이루어지며, FinGPT와 Bloomber...   \n",
       "8   금융 분야에서 LLM 파인튜닝이 어떻게 이루어지고 있으며, 어떤 모델들이 사용되고 ...   \n",
       "9       금융 분야에서 모델 성능 극대화를 위해 하이퍼파라미터 조정은 어떻게 이루어지나요?   \n",
       "10  금융 분야에서 LLM의 파인튜닝은 어떤 모델들이 있으며, 그들의 성능은 어떻게 평가...   \n",
       "11         금융 분야에서 모델의 용도에 따라 하이퍼파라미터 조정은 어떻게 이루어지나요?   \n",
       "12  금융 분야에서 LLM 파인튜닝의 중요성과 FinGPT, BloombergGPT, F...   \n",
       "13  금융 분야에서 모델의 용도에 따라 하이퍼파라미터 조정이 어떻게 이루어지며, 금융 특...   \n",
       "14                FinGPT는 금융 분야에서 어떤 특화된 기능을 가지고 있나요?   \n",
       "15  FinGPT는 금융 분야에서 어떤 방식으로 파인튜닝되어 있으며, BloombergG...   \n",
       "16               GPT-4와 FinGPT의 금융 도메인에서의 차이점은 무엇인가요?   \n",
       "17      FinBERT와 BloombergGPT의 금융 분야에서의 성능 차이는 무엇인가요?   \n",
       "18   BloombergGPT와 BLOOM 모델의 금융 특화 파인튜닝은 어떤 차이점이 있나요?   \n",
       "19     BloombergGPT와 BLOOM 모델의 금융 분야에서의 성능 차이는 무엇인가요?   \n",
       "20   Mistral-7B SLM의 금융 분야에서의 활용 가능성과 파인튜닝 방법은 무엇인가요?   \n",
       "21  Mistral-7B SLM의 금융 데이터에 대한 파인튜닝은 어떻게 이루어지며, 이 ...   \n",
       "\n",
       "                                   reference_contexts  \\\n",
       "0   [<표 7> Evaluation Benchmarks of BloombergGPT. ...   \n",
       "1   [하여, 금융 분야의 특정 작업에 높은 성능을 3.4.2. 금융 특화 파인튜닝 LL...   \n",
       "2   [금융 데이터의 특성을 고려한 파인튜닝을 통해 된 데이터셋은 생성방법을 가이드하기위...   \n",
       "3   [3.4. 금융 분야를 위한 LLM 구성 3.3.3. 보안 및 규정 준수 금융 분야...   \n",
       "4   [수 있다. 예를 들어, 모델은 금융 뉴스, 기업 보고 4.2.4. 자동화된 금융 ...   \n",
       "5   [사전학습하기 위한 사례에 맞게 파인튜닝을 하게 된다(정천수, 2023d). <그림...   \n",
       "6   [고객 응대 개선측 종합적으로, 본 연구는 LLM을 금융 분야에 면에서는 자동 응답...   \n",
       "7   [<1-hop>\\n\\n하여, 금융 분야의 특정 작업에 높은 성능을 3.4.2. 금융...   \n",
       "8   [<1-hop>\\n\\n하여, 금융 분야의 특정 작업에 높은 성능을 3.4.2. 금융...   \n",
       "9   [<1-hop>\\n\\n2) 모델의 용도: 모델의 용도에 따라 적합한 3.2.2. 하...   \n",
       "10  [<1-hop>\\n\\n하여, 금융 분야의 특정 작업에 높은 성능을 3.4.2. 금융...   \n",
       "11  [<1-hop>\\n\\n2) 모델의 용도: 모델의 용도에 따라 적합한 3.2.2. 하...   \n",
       "12  [<1-hop>\\n\\n하여, 금융 분야의 특정 작업에 높은 성능을 3.4.2. 금융...   \n",
       "13  [<1-hop>\\n\\n2) 모델의 용도: 모델의 용도에 따라 적합한 3.2.2. 하...   \n",
       "14  [<1-hop>\\n\\n3.4. 금융 분야를 위한 LLM 구성 3.3.3. 보안 및 ...   \n",
       "15  [<1-hop>\\n\\n3.4. 금융 분야를 위한 LLM 구성 3.3.3. 보안 및 ...   \n",
       "16  [<1-hop>\\n\\n모델의 가용성: 모델이 공개되어 있지 않다 적절히 조정함으로써...   \n",
       "17  [<1-hop>\\n\\n모델의 가용성: 모델이 공개되어 있지 않다 적절히 조정함으로써...   \n",
       "18  [<1-hop>\\n\\n<표 7> Evaluation Benchmarks of Blo...   \n",
       "19  [<1-hop>\\n\\n<표 7> Evaluation Benchmarks of Blo...   \n",
       "20  [<1-hop>\\n\\n모델의 가용성: 모델이 공개되어 있지 않다 적절히 조정함으로써...   \n",
       "21  [<1-hop>\\n\\n모델의 가용성: 모델이 공개되어 있지 않다 적절히 조정함으로써...   \n",
       "\n",
       "                                            reference  \\\n",
       "0   BloombergGPT uses several evaluation benchmark...   \n",
       "1   금융 특화 파인튜닝은 금융 분야의 특정 작업에 높은 성능을 보이는 LLM(대형 언어...   \n",
       "2   금융 리서치에서 LLM을 활용하는 방법은 다양한 금융 데이터셋을 확보하여 학습하고,...   \n",
       "3   FinGPT는 금융 분야에서 자연어 처리에 특화된 모델로, 금융 데이터의 민감한 정...   \n",
       "4   미래에셋증권은 챗 GPT를 활용해 종목의 시황을 요약하는 서비스 작성을 도입하여 금...   \n",
       "5        정천수는 사전학습하기 위한 사례에 맞게 파인튜닝을 하게 되는 과정을 나타냅니다.   \n",
       "6   자동 응답 시스템은 LLM을 금융 분야에 적용하여 고객 응대의 품질을 향상시키는 데...   \n",
       "7   금융 분야에서 LLM의 파인튜닝은 특정 작업에 높은 성능을 보이기 위해 금융 특화된...   \n",
       "8   금융 분야에서 LLM 파인튜닝은 특정 작업에 높은 성능을 보이기 위해 금융 특화된 ...   \n",
       "9   금융 분야에서 모델 성능 극대화를 위해 하이퍼파라미터 조정은 모델의 용도에 따라 적...   \n",
       "10  금융 분야에서 LLM의 파인튜닝은 여러 모델들이 있으며, 그 중 FinGPT와 Fi...   \n",
       "11  금융 분야에서 모델의 용도에 따라 하이퍼파라미터 조정은 모델의 성능을 극대화하기 위...   \n",
       "12  금융 분야에서 LLM 파인튜닝은 특정 작업에 높은 성능을 보이는 모델을 개발하는 데...   \n",
       "13  모델의 용도에 따라 적합한 하이퍼파라미터 조정이 이루어지며, 예를 들어 금융 뉴스 ...   \n",
       "14  FinGPT는 금융 도메인에 특화된 데이터셋과 파인튜닝을 통해 높은 성능을 달성하며...   \n",
       "15  FinGPT는 OpenAI의 GPT 아키텍처를 기반으로 하여 금융 도메인에서 처음부...   \n",
       "16  GPT-4는 OpenAI에서 개발한 대규모 언어 모델로, 1,750억 개 이상의 매...   \n",
       "17  FinBERT는 BERT 아키텍처를 사용하여 금융 데이터로 사전 학습되었으며, 주로...   \n",
       "18  BloombergGPT는 금융 전문 언어를 학습한 GPT 기반 모델로, 예측 분석 ...   \n",
       "19  BloombergGPT는 금융 전문 언어를 학습한 GPT 기반 모델로, 예측 분석과...   \n",
       "20  Mistral-7B SLM은 금융 분야에서의 다양한 작업에 활용될 수 있으며, 파인...   \n",
       "21  Mistral-7B SLM의 금융 데이터에 대한 파인튜닝은 금융 데이터의 특성을 고...   \n",
       "\n",
       "                        synthesizer_name  validity  \\\n",
       "0   single_hop_specifc_query_synthesizer      True   \n",
       "1   single_hop_specifc_query_synthesizer      True   \n",
       "2   single_hop_specifc_query_synthesizer      True   \n",
       "3   single_hop_specifc_query_synthesizer      True   \n",
       "4   single_hop_specifc_query_synthesizer      True   \n",
       "5   single_hop_specifc_query_synthesizer     False   \n",
       "6   single_hop_specifc_query_synthesizer      True   \n",
       "7   multi_hop_abstract_query_synthesizer      True   \n",
       "8   multi_hop_abstract_query_synthesizer      True   \n",
       "9   multi_hop_abstract_query_synthesizer      True   \n",
       "10  multi_hop_abstract_query_synthesizer      True   \n",
       "11  multi_hop_abstract_query_synthesizer      True   \n",
       "12  multi_hop_abstract_query_synthesizer      True   \n",
       "13  multi_hop_abstract_query_synthesizer      True   \n",
       "14  multi_hop_specific_query_synthesizer      True   \n",
       "15  multi_hop_specific_query_synthesizer      True   \n",
       "16  multi_hop_specific_query_synthesizer      True   \n",
       "17  multi_hop_specific_query_synthesizer      True   \n",
       "18  multi_hop_specific_query_synthesizer      True   \n",
       "19  multi_hop_specific_query_synthesizer      True   \n",
       "20  multi_hop_specific_query_synthesizer      True   \n",
       "21  multi_hop_specific_query_synthesizer      True   \n",
       "\n",
       "    reference_context_relevance  qa_appropriateness  \n",
       "0                          True                True  \n",
       "1                          True                True  \n",
       "2                          True                True  \n",
       "3                          True                True  \n",
       "4                          True                True  \n",
       "5                          True               False  \n",
       "6                          True                True  \n",
       "7                          True                True  \n",
       "8                          True                True  \n",
       "9                          True                True  \n",
       "10                         True                True  \n",
       "11                         True                True  \n",
       "12                         True                True  \n",
       "13                         True                True  \n",
       "14                         True                True  \n",
       "15                         True                True  \n",
       "16                         True                True  \n",
       "17                         True                True  \n",
       "18                         True                True  \n",
       "19                         True                True  \n",
       "20                         True                True  \n",
       "21                         True                True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.concat([sample_df,pd.DataFrame([i.model_dump() for i in ans_total])], axis=1).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAGAS 라이브러리와 형식 통합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.prompt.pydantic_prompt import PydanticPrompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QA_check_input(BaseModel):\n",
    "    question: str\n",
    "    answer: str\n",
    "    reference_context: t.List[str]\n",
    "\n",
    "class QA_check_output(BaseModel):\n",
    "    validity: bool\n",
    "    reference_context_relevance: bool\n",
    "    qa_appropriateness:bool "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing as t\n",
    "class QA_checkPrompt(PydanticPrompt[QA_check_input, QA_check_output]):\n",
    "    instruction: str = (\n",
    "        \"Evaluate whether the following question and answer are appropriate based on the provided paper.\"\n",
    "        \"Please sequentially assess the following items:\"\n",
    "\n",
    "        \"1. Question Validity\"\n",
    "        \"- Is the question clearly structured based on the terms, expressions, or concepts mentioned in the paper?\"\n",
    "        \"- It is inappropriate if the question misinterprets an author's name or citation (e.g., (Hong, 2024)) as a term or concept.\"\n",
    "\n",
    "        \"2. Reference Context Relevance\"\n",
    "        \"- Does the provided reference context accurately match the actual content of the paper and relate properly to the question and answer?\"\n",
    "\n",
    "        \"3. QA Appropriateness\"\n",
    "        \"- Does the provided answer appropriately respond to the question based on information explicitly stated in the paper?\"\n",
    "    )\n",
    "    input_model: t.Type[QA_check_input] = QA_check_input\n",
    "    output_model: t.Type[QA_check_output] = QA_check_output\n",
    "    examples: t.List[t.Tuple[QA_check_input, QA_check_output]] = [\n",
    "        (\n",
    "            QA_check_input(\n",
    "                question=\"What impact do few-shot prompting techniques have on LLM models?\",\n",
    "                answer=\"Few-shot prompting techniques help improve the question-answering performance of LLM models.\",\n",
    "                reference_context=[\"Recent LLM models are known to improve question-answering performance using few-shot prompting techniques (Lee, 2023).\"],\n",
    "            ),\n",
    "            QA_check_output(\n",
    "                validity=True,\n",
    "                reference_context_relevance=True,\n",
    "                qa_appropriateness=True\n",
    "            )\n",
    "        ),\n",
    "        (\n",
    "            QA_check_input(\n",
    "                question=\"What does Kim mean?\",\n",
    "                answer=\"Kim refers to the fine-tuning method of further training models on specific tasks.\",\n",
    "                reference_context=[\"Fine-tuning is a methodology to improve performance by further training pre-trained models on specific tasks (Kim, 2022).\"],\n",
    "            ),\n",
    "            QA_check_output(\n",
    "                validity=False,\n",
    "                reference_context_relevance=False,\n",
    "                qa_appropriateness=False\n",
    "            )\n",
    "        )\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "introduction_prompt_kor = \"\"\"\"\n",
    "다음 질문과 답변이 주어진 논문을 기준으로 적절한지 평가하십시오.\n",
    "\n",
    "다음 항목을 순차적으로 평가해 주세요:\n",
    "\n",
    "1. 질문의 적절성 (Validity)\n",
    "- 질문이 논문에 언급된 용어, 표현, 개념에 대해 논문에서 제공하는 정보를 바탕으로 답할 수 있도록 명확히 구성되었나요?\n",
    "- 논문에서 인용된 사람의 이름 또는 특정 저자명을 단순 인용 표기(예: (홍길동, 2024))임에도 불구하고 단어나 개념처럼 잘못 해석하여 그 의미를 묻는 질문은 부적절합니다.\n",
    "(예: 논문 인용 표기에서 \"(홍길동, 2024)\"와 같은 저자 이름을 하나의 개념이나 용어로 잘못 파악하고 질문하는 경우는 잘못된 질문임.)\n",
    "\n",
    "2. 참조 문맥의 관련성 (Reference Context Relevance)\n",
    "- 제공된 참조 문맥이 질문과 답변에 관련 있으며, 논문의 실제 내용과 정확히 일치하는가요?\n",
    "\n",
    "3. 답변의 적합성 (QA Appropriateness)\n",
    "- 제공된 답변이 논문에서 실제로 제시된 정보를 근거로 질문에 적절히 대응하는가요?\" \\\n",
    "\"\"\"\n",
    "\n",
    "example_kor = [\n",
    "        (\n",
    "            QA_check_input(\n",
    "                question=\"Few-shot 프롬프트 기법이 LLM 모델에 미치는 영향은 무엇인가요?\",\n",
    "                answer=\"Few-shot 프롬프트 기법은 LLM 모델의 질문-답변 성능을 향상시키는 데 도움을 줍니다.\",\n",
    "                reference_context=[\"최근 LLM 모델은 Few-shot 프롬프트 기법을 활용하여 질문-답변 성능을 향상시킬 수 있다고 알려졌다(AAA, 2023).\"],\n",
    "            ),\n",
    "            QA_check_output(\n",
    "                validity=True,\n",
    "                reference_context_relevance=True,\n",
    "                qa_appropriateness=True\n",
    "            )\n",
    "        ),\n",
    "        (\n",
    "            QA_check_input(\n",
    "                question=\"BBB는 무엇을 의미하나요?\",\n",
    "                answer=\"김철수는 특정 작업에 맞게 모델을 추가 학습하는 파인튜닝 방법을 의미합니다.\",\n",
    "                reference_context=[\"파인튜닝은 특정 작업에 맞게 사전 학습된 모델을 추가 학습하여 성능을 개선하는 방법론이다(BBB, 2022).\"],\n",
    "            ),\n",
    "            QA_check_output(\n",
    "                validity=False,\n",
    "                reference_context_relevance=False,\n",
    "                qa_appropriateness=False\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    "\n",
    "qa_check = QA_checkPrompt()\n",
    "qa_check.instruction = introduction_prompt_kor\n",
    "qa_check.examples = example_kor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SingleTurnSample(user_input='What evaluation benchmarks are used for BloombergGPT, and what specific tasks does it measure in the financial domain?', retrieved_contexts=None, reference_contexts=['<표 7> Evaluation Benchmarks of BloombergGPT. Suit Tasks What does it measure? Public Financial Tasks 5 Public datasets in the financial domain Bloomberg Financial Tasks 12 NER and sentiment analysis tasks Big-bench Hard (Suzgun et al., 2022) 23 Reasoning and general NLP tasks Knowledge Assessments 5 Testing closed-book information recall Reading Comprehension 5 Testing open-book tasks Linguistic Tasks 9 Not directly user-facing NLP tasks 109'], response=None, multi_responses=None, reference='BloombergGPT uses several evaluation benchmarks, including Public Financial Tasks, which measure 5 public datasets in the financial domain, and Bloomberg Financial Tasks, which consist of 12 NER and sentiment analysis tasks. Additionally, it includes Big-bench Hard (Suzgun et al., 2022) with 23 reasoning and general NLP tasks, Knowledge Assessments with 5 testing closed-book information recall tasks, Reading Comprehension with 5 testing open-book tasks, and Linguistic Tasks, which encompass 9 not directly user-facing NLP tasks, totaling 109 tasks.', rubrics=None)"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples.to_evaluation_dataset()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample2qainput(sample):\n",
    "    return QA_check_input(\n",
    "        question=sample.user_input,\n",
    "        answer=sample.reference,\n",
    "        reference_context=sample.reference_contexts\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QA_check_input(question='금융 특화 파인튜닝이란 무엇이며, 어떤 성능을 보이나요?', answer='금융 특화 파인튜닝은 금융 분야의 특정 작업에 높은 성능을 보이는 LLM(대형 언어 모델)으로, 금융 분야에 특화된 파인튜닝이 적용된 모델입니다.', reference_context=['하여, 금융 분야의 특정 작업에 높은 성능을 3.4.2. 금융 특화 파인튜닝 LLM 보이고 있다. (LLM fine-tuned for finance) 금융 분야에 특화된 파인튜닝이 적용된 모델 '])"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample2qainput(samples.to_evaluation_dataset()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = await qa_check.generate(data=sample2qainput(samples.to_evaluation_dataset()[5]), llm=generator_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QA_check_output(validity=False, reference_context_relevance=True, qa_appropriateness=False)"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
